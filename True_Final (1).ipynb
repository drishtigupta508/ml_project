{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THTONAMw3Tvr",
        "outputId": "a0793b37-6202-49e1-fe15-02640bbc29b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2WvIwOQWY_Vc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Import all regression models covered in class\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    AdaBoostRegressor,\n",
        "    BaggingRegressor,\n",
        "    StackingRegressor,\n",
        "    VotingRegressor\n",
        ")\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Raw Data\n",
        "\n",
        "We pull in the raw test and train cattle datasets. This step is about making sure the files are loaded correctly, checking the shapes, skimming the first few rows, and confirming the data types of the dataset."
      ],
      "metadata": {
        "id": "qkr2nEuUAoJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SpiIuFIcN1B",
        "outputId": "1256b90c-d8f7-4036-dcbc-134b4864bb63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (210000, 36)\n",
            "Test data shape: (40000, 35)\n",
            "\n",
            "First few rows of training data:\n",
            "       Cattle_ID     Breed   Climate_Zone Management_System  Age_Months  \\\n",
            "0  CATTLE_133713  Holstein       Tropical         Intensive         114   \n",
            "1  CATTLE_027003  Holstein           Arid             Mixed         136   \n",
            "2  CATTLE_122459  Holstein       Tropical    Semi_Intensive          64   \n",
            "3  CATTLE_213419    Jersey  Mediterranean         Intensive          58   \n",
            "4  CATTLE_106260  Guernsey    Subtropical         Intensive          84   \n",
            "\n",
            "   Weight_kg  Parity Lactation_Stage  Days_in_Milk      Feed_Type  ...  \\\n",
            "0      544.8       4             Mid            62   Concentrates  ...   \n",
            "1      298.9       4             Mid           213  Crop_Residues  ...   \n",
            "2      336.6       4            Late            16            Hay  ...   \n",
            "3      370.5       1           Early           339  Crop_Residues  ...   \n",
            "4      641.5       6           Early           125     Mixed_Feed  ...   \n",
            "\n",
            "   BVD_Vaccine  Rabies_Vaccine  Previous_Week_Avg_Yield  Body_Condition_Score  \\\n",
            "0            0               1                     6.31                   3.0   \n",
            "1            0               0                    17.16                   4.0   \n",
            "2            1               0                     4.07                   3.5   \n",
            "3            0               0                    10.23                   3.0   \n",
            "4            1               1                    20.68                   3.0   \n",
            "\n",
            "   Milking_Interval_hrs        Date    Farm_ID  Feed_Quantity_lb  Mastitis  \\\n",
            "0                    12  2024-01-15  FARM_0301           36.8235         1   \n",
            "1                    12  2023-10-31  FARM_0219               NaN         0   \n",
            "2                    12  2024-05-20  FARM_0802           16.0965         0   \n",
            "3                    24  2024-07-22  FARM_0034           40.7925         0   \n",
            "4                    12  2023-01-03  FARM_0695           33.7365         1   \n",
            "\n",
            "   Milk_Yield_L  \n",
            "0     12.192634  \n",
            "1     14.717031  \n",
            "2     14.006142  \n",
            "3     24.324325  \n",
            "4     12.023074  \n",
            "\n",
            "[5 rows x 36 columns]\n",
            "\n",
            "Data types:\n",
            "Cattle_ID                   object\n",
            "Breed                       object\n",
            "Climate_Zone                object\n",
            "Management_System           object\n",
            "Age_Months                   int64\n",
            "Weight_kg                  float64\n",
            "Parity                       int64\n",
            "Lactation_Stage             object\n",
            "Days_in_Milk                 int64\n",
            "Feed_Type                   object\n",
            "Feed_Quantity_kg           float64\n",
            "Feeding_Frequency            int64\n",
            "Water_Intake_L             float64\n",
            "Walking_Distance_km        float64\n",
            "Grazing_Duration_hrs       float64\n",
            "Rumination_Time_hrs        float64\n",
            "Resting_Hours              float64\n",
            "Ambient_Temperature_C      float64\n",
            "Humidity_percent           float64\n",
            "Housing_Score              float64\n",
            "FMD_Vaccine                  int64\n",
            "Brucellosis_Vaccine          int64\n",
            "HS_Vaccine                   int64\n",
            "BQ_Vaccine                   int64\n",
            "Anthrax_Vaccine              int64\n",
            "IBR_Vaccine                  int64\n",
            "BVD_Vaccine                  int64\n",
            "Rabies_Vaccine               int64\n",
            "Previous_Week_Avg_Yield    float64\n",
            "Body_Condition_Score       float64\n",
            "Milking_Interval_hrs         int64\n",
            "Date                        object\n",
            "Farm_ID                     object\n",
            "Feed_Quantity_lb           float64\n",
            "Mastitis                     int64\n",
            "Milk_Yield_L               float64\n",
            "dtype: object\n",
            "\n",
            "Column names:\n",
            "['Cattle_ID', 'Breed', 'Climate_Zone', 'Management_System', 'Age_Months', 'Weight_kg', 'Parity', 'Lactation_Stage', 'Days_in_Milk', 'Feed_Type', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Rumination_Time_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Date', 'Farm_ID', 'Feed_Quantity_lb', 'Mastitis', 'Milk_Yield_L']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load test and train cattle datasets\n",
        "train_data = pd.read_csv('sample_data/cattle_data_train.csv')\n",
        "test_data = pd.read_csv('sample_data/cattle_data_test.csv')\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print()\n",
        "\n",
        "# Display first few rows of the data\n",
        "print(\"First few rows of training data:\")\n",
        "print(train_data.head())\n",
        "print()\n",
        "\n",
        "# Display data types\n",
        "print(\"Data types:\")\n",
        "print(train_data.dtypes)\n",
        "print()\n",
        "\n",
        "# Inspect column names\n",
        "print(\"Column names:\")\n",
        "print(train_data.columns.tolist())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "zOFF436LCGBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdZ_nGwZ3Tvs",
        "outputId": "2913f241-b0ac-4dea-aef0-3cb8f1fbdc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Milking_Interval_hrs:\n",
            "0         12\n",
            "1         12\n",
            "2         12\n",
            "3         24\n",
            "4         12\n",
            "          ..\n",
            "209995    12\n",
            "209996    12\n",
            "209997    24\n",
            "209998    12\n",
            "209999    12\n",
            "Name: Milking_Interval_hrs, Length: 210000, dtype: int64\n",
            "\n",
            "Basic statistics:\n",
            "  count:   210000\n",
            "  missing: 0\n",
            "  unique:  4\n",
            "  min:     6\n",
            "  max:     24\n",
            "  mean:    12.3024\n",
            "  median:  12.0000\n",
            "  std:     4.2990\n",
            "  mode(s): [12]\n",
            "\n",
            "Unique values:\n",
            "[6, 8, 12, 24]\n",
            "\n",
            "Quantiles:\n",
            "   5th: 8.0\n",
            "  25th: 12.0\n",
            "  50th: 12.0\n",
            "  75th: 12.0\n",
            "  95th: 24.0\n",
            "\n",
            "Top 10 most frequent values:\n",
            "Milking_Interval_hrs\n",
            "12    147167\n",
            "8      31399\n",
            "24     20984\n",
            "6      10450\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Shiv Testing\n",
        "print(\"Train Milking_Interval_hrs:\")\n",
        "series = train_data['Milking_Interval_hrs']\n",
        "print(series)\n",
        "\n",
        "\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(f\"  count:   {series.count()}\")\n",
        "print(f\"  missing: {series.isna().sum()}\")\n",
        "print(f\"  unique:  {series.nunique()}\")\n",
        "print(f\"  min:     {series.min()}\")\n",
        "print(f\"  max:     {series.max()}\")\n",
        "print(f\"  mean:    {series.mean():.4f}\")\n",
        "print(f\"  median:  {series.median():.4f}\")\n",
        "print(f\"  std:     {series.std():.4f}\")\n",
        "modes = series.mode().tolist()\n",
        "print(f\"  mode(s): {modes}\")\n",
        "\n",
        "print(\"\\nUnique values:\")\n",
        "print(sorted(series.dropna().unique().tolist()))\n",
        "\n",
        "print(\"\\nQuantiles:\")\n",
        "quantiles = series.quantile([0.05, 0.25, 0.5, 0.75, 0.95])\n",
        "for q, v in quantiles.items():\n",
        "    print(f\"  {int(q*100):>2}th: {v}\")\n",
        "\n",
        "print(\"\\nTop 10 most frequent values:\")\n",
        "print(series.value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a feel for the dataset, we checked the structure of the data to spot missing values and understand how the target var (Milk_Yield_L) behaves. We also look at the distribution of the target variable using a histogram and box plot, helping us understand how the data is skewed and whether or not outliers might cause any issues during model training."
      ],
      "metadata": {
        "id": "4gQKsTTdCnGq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ASGyzbcOzM",
        "outputId": "6967cfcd-62dd-4c25-eea6-d3b68e85a5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Overview\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 210000 entries, 0 to 209999\n",
            "Data columns (total 36 columns):\n",
            " #   Column                   Non-Null Count   Dtype  \n",
            "---  ------                   --------------   -----  \n",
            " 0   Cattle_ID                210000 non-null  object \n",
            " 1   Breed                    210000 non-null  object \n",
            " 2   Climate_Zone             210000 non-null  object \n",
            " 3   Management_System        210000 non-null  object \n",
            " 4   Age_Months               210000 non-null  int64  \n",
            " 5   Weight_kg                210000 non-null  float64\n",
            " 6   Parity                   210000 non-null  int64  \n",
            " 7   Lactation_Stage          210000 non-null  object \n",
            " 8   Days_in_Milk             210000 non-null  int64  \n",
            " 9   Feed_Type                210000 non-null  object \n",
            " 10  Feed_Quantity_kg         199519 non-null  float64\n",
            " 11  Feeding_Frequency        210000 non-null  int64  \n",
            " 12  Water_Intake_L           210000 non-null  float64\n",
            " 13  Walking_Distance_km      210000 non-null  float64\n",
            " 14  Grazing_Duration_hrs     210000 non-null  float64\n",
            " 15  Rumination_Time_hrs      210000 non-null  float64\n",
            " 16  Resting_Hours            210000 non-null  float64\n",
            " 17  Ambient_Temperature_C    210000 non-null  float64\n",
            " 18  Humidity_percent         210000 non-null  float64\n",
            " 19  Housing_Score            203721 non-null  float64\n",
            " 20  FMD_Vaccine              210000 non-null  int64  \n",
            " 21  Brucellosis_Vaccine      210000 non-null  int64  \n",
            " 22  HS_Vaccine               210000 non-null  int64  \n",
            " 23  BQ_Vaccine               210000 non-null  int64  \n",
            " 24  Anthrax_Vaccine          210000 non-null  int64  \n",
            " 25  IBR_Vaccine              210000 non-null  int64  \n",
            " 26  BVD_Vaccine              210000 non-null  int64  \n",
            " 27  Rabies_Vaccine           210000 non-null  int64  \n",
            " 28  Previous_Week_Avg_Yield  210000 non-null  float64\n",
            " 29  Body_Condition_Score     210000 non-null  float64\n",
            " 30  Milking_Interval_hrs     210000 non-null  int64  \n",
            " 31  Date                     210000 non-null  object \n",
            " 32  Farm_ID                  210000 non-null  object \n",
            " 33  Feed_Quantity_lb         199519 non-null  float64\n",
            " 34  Mastitis                 210000 non-null  int64  \n",
            " 35  Milk_Yield_L             210000 non-null  float64\n",
            "dtypes: float64(14), int64(14), object(8)\n",
            "memory usage: 57.7+ MB\n",
            "\n",
            "          Age_Months      Weight_kg         Parity   Days_in_Milk  \\\n",
            "count  210000.000000  210000.000000  210000.000000  210000.000000   \n",
            "mean       83.483905     499.930430       3.500395     182.112967   \n",
            "std        34.648982     144.659172       1.707383     105.051486   \n",
            "min        24.000000     250.000000       1.000000       1.000000   \n",
            "25%        54.000000     374.200000       2.000000      91.000000   \n",
            "50%        83.000000     500.200000       3.000000     182.000000   \n",
            "75%       114.000000     625.700000       5.000000     273.000000   \n",
            "max       143.000000     750.000000       6.000000     364.000000   \n",
            "\n",
            "       Feed_Quantity_kg  Feeding_Frequency  Water_Intake_L  \\\n",
            "count     199519.000000      210000.000000   210000.000000   \n",
            "mean          12.014793           2.999119       80.036850   \n",
            "std            3.969247           1.413147       14.987677   \n",
            "min            2.370284           1.000000       14.207737   \n",
            "25%            9.283265           2.000000       69.919162   \n",
            "50%           12.002254           3.000000       80.016973   \n",
            "75%           14.708920           4.000000       90.119812   \n",
            "max           25.454207           5.000000      149.960210   \n",
            "\n",
            "       Walking_Distance_km  Grazing_Duration_hrs  Rumination_Time_hrs  ...  \\\n",
            "count        210000.000000         210000.000000        210000.000000  ...   \n",
            "mean              4.034754              6.056710             0.256557  ...   \n",
            "std               1.928529              2.867575             6.115351  ...   \n",
            "min               0.500000              1.000000            -8.808053  ...   \n",
            "25%               2.650000              4.000000            -4.383302  ...   \n",
            "50%               4.000000              6.000000            -0.818631  ...   \n",
            "75%               5.350000              8.000000             4.051704  ...   \n",
            "max              12.000000             14.000000            31.263406  ...   \n",
            "\n",
            "       Anthrax_Vaccine    IBR_Vaccine    BVD_Vaccine  Rabies_Vaccine  \\\n",
            "count    210000.000000  210000.000000  210000.000000   210000.000000   \n",
            "mean          0.600381       0.598814       0.599824        0.600824   \n",
            "std           0.489821       0.490140       0.489935        0.489730   \n",
            "min           0.000000       0.000000       0.000000        0.000000   \n",
            "25%           0.000000       0.000000       0.000000        0.000000   \n",
            "50%           1.000000       1.000000       1.000000        1.000000   \n",
            "75%           1.000000       1.000000       1.000000        1.000000   \n",
            "max           1.000000       1.000000       1.000000        1.000000   \n",
            "\n",
            "       Previous_Week_Avg_Yield  Body_Condition_Score  Milking_Interval_hrs  \\\n",
            "count            210000.000000         210000.000000         210000.000000   \n",
            "mean                  8.747584              3.394726             12.302438   \n",
            "std                   5.901473              0.632831              4.298998   \n",
            "min                   0.000000              2.000000              6.000000   \n",
            "25%                   4.270000              3.000000             12.000000   \n",
            "50%                   7.710000              3.500000             12.000000   \n",
            "75%                  12.410000              4.000000             12.000000   \n",
            "max                  38.670000              5.000000             24.000000   \n",
            "\n",
            "       Feed_Quantity_lb       Mastitis   Milk_Yield_L  \n",
            "count     199519.000000  210000.000000  210000.000000  \n",
            "mean          26.492720       0.099976      15.589156  \n",
            "std            8.741282       0.299969       5.352079  \n",
            "min            6.615000       0.000000      -5.700324  \n",
            "25%           20.506500       0.000000      11.822207  \n",
            "50%           26.460000       0.000000      15.145871  \n",
            "75%           32.413500       0.000000      18.884708  \n",
            "max           55.125000       1.000000      44.555285  \n",
            "\n",
            "[8 rows x 28 columns]\n",
            "\n",
            "1.2 Missing Values Analysis\n",
            "--------------------------------------------------------------------------------\n",
            "                  Missing_Count  Percentage\n",
            "Feed_Quantity_kg          10481    4.990952\n",
            "Feed_Quantity_lb          10481    4.990952\n",
            "Housing_Score              6279    2.990000\n",
            "\n",
            "✓ Missing data visualization saved as 'missing_data.png'\n",
            "\n",
            "1.3 Target Variable (Milk_Yield_L) Distribution\n",
            "--------------------------------------------------------------------------------\n",
            "Mean:   15.59 L\n",
            "Median: 15.15 L\n",
            "Std:    5.35 L\n",
            "Min:    -5.70 L\n",
            "Max:    44.56 L\n",
            "Skewness: 0.48\n",
            "\n",
            "✓ Target distribution saved as 'target_distribution.png'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Basic Dataset Info\n",
        "print(\"Dataset Overview\")\n",
        "print(\"-\" * 80)\n",
        "print(train_data.describe())\n",
        "print()\n",
        "\n",
        "# Check for Missing Values\n",
        "print(\"Missing Values Analysis\")\n",
        "print(\"-\" * 80)\n",
        "missing_counts = train_data.isnull().sum()\n",
        "missing_pct = 100 * train_data.isnull().sum() / len(train_data)\n",
        "missing_table = pd.DataFrame({\n",
        "    'Missing_Count': missing_counts,\n",
        "    'Percentage': missing_pct\n",
        "})\n",
        "missing_table = missing_table[missing_table['Missing_Count'] > 0].sort_values(\n",
        "    'Percentage', ascending=False\n",
        ")\n",
        "print(missing_table)\n",
        "print()\n",
        "\n",
        "# Visualize missing data pattern\n",
        "if len(missing_table) > 0:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    missing_table['Percentage'].plot(kind='barh')\n",
        "    plt.xlabel('Percentage Missing')\n",
        "    plt.title('Missing Data by Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('missing_data.png')\n",
        "    plt.close()\n",
        "    print(\"✓ Missing data visualization saved as 'missing_data.png'\")\n",
        "print()\n",
        "\n",
        "# Target Variable Distribution\n",
        "print(\"1.3 Target Variable (Milk_Yield_L) Distribution\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Mean:   {train_data['Milk_Yield_L'].mean():.2f} L\")\n",
        "print(f\"Median: {train_data['Milk_Yield_L'].median():.2f} L\")\n",
        "print(f\"Std:    {train_data['Milk_Yield_L'].std():.2f} L\")\n",
        "print(f\"Min:    {train_data['Milk_Yield_L'].min():.2f} L\")\n",
        "print(f\"Max:    {train_data['Milk_Yield_L'].max():.2f} L\")\n",
        "print(f\"Skewness: {train_data['Milk_Yield_L'].skew():.2f}\")\n",
        "print()\n",
        "\n",
        "# Plot distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].hist(train_data['Milk_Yield_L'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Milk Yield (L)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution of Milk Yield')\n",
        "axes[0].axvline(train_data['Milk_Yield_L'].mean(), color='r', linestyle='--', label='Mean')\n",
        "axes[0].axvline(train_data['Milk_Yield_L'].median(), color='g', linestyle='--', label='Median')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].boxplot(train_data['Milk_Yield_L'])\n",
        "axes[1].set_ylabel('Milk Yield (L)')\n",
        "axes[1].set_title('Box Plot of Milk Yield')\n",
        "plt.tight_layout()\n",
        "plt.savefig('target_distribution.png')\n",
        "plt.close()\n",
        "print(\"✓ Target distribution saved as 'target_distribution.png'\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Types, Dates, Correlations\n",
        "\n",
        "To understand what features we're dealing with, we separate the categorical and numerical columns. The dataset has vaccine-related columns — we grouped these in with the categorical.\n",
        "\n",
        "For the numeric columns, we check if there are any redundant features. In this dataset, Feed_Quantity_kg and Feed_Quantity_lb represent the same thing but in different units, so we confirmed the ratio to be sure.\n",
        "\n",
        "We also observed the date column and converted it to a proper datetime. This allows us to more easily extract what we want from the date later on (during feature engineering).\n",
        "\n",
        "Finally, we ran a correlation analysis to see what numeric features actually relate to milk production. This gives us an idea of what variables might be useful and which ones might be noise."
      ],
      "metadata": {
        "id": "JZ1Z3_zNHCbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlG9EMZXckt7",
        "outputId": "01541cbd-49fc-441f-ee15-925b30447dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Type Identification\n",
            "--------------------------------------------------------------------------------\n",
            "Found 8 vaccine columns: ['FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine']\n",
            "\n",
            "Numeric features (18):\n",
            "  • Age_Months\n",
            "  • Weight_kg\n",
            "  • Parity\n",
            "  • Days_in_Milk\n",
            "  • Feed_Quantity_kg\n",
            "  • Feeding_Frequency\n",
            "  • Water_Intake_L\n",
            "  • Walking_Distance_km\n",
            "  • Grazing_Duration_hrs\n",
            "  • Rumination_Time_hrs\n",
            "  • Resting_Hours\n",
            "  • Ambient_Temperature_C\n",
            "  • Humidity_percent\n",
            "  • Housing_Score\n",
            "  • Previous_Week_Avg_Yield\n",
            "  • Body_Condition_Score\n",
            "  • Milking_Interval_hrs\n",
            "  • Feed_Quantity_lb\n",
            "\n",
            "Categorical features (14):\n",
            "  • Breed\n",
            "  • Climate_Zone\n",
            "  • Management_System\n",
            "  • Lactation_Stage\n",
            "  • Feed_Type\n",
            "  • Mastitis\n",
            "  • FMD_Vaccine\n",
            "  • Brucellosis_Vaccine\n",
            "  • HS_Vaccine\n",
            "  • BQ_Vaccine\n",
            "  • Anthrax_Vaccine\n",
            "  • IBR_Vaccine\n",
            "  • BVD_Vaccine\n",
            "  • Rabies_Vaccine\n",
            "\n",
            "Checking Feed Quantity Columns\n",
            "--------------------------------------------------------------------------------\n",
            "Average lb/kg ratio: 2.2060 (expected: 2.20462)\n",
            "\n",
            "Date Feature Analysis\n",
            "--------------------------------------------------------------------------------\n",
            "Date range: 2022-01-01 00:00:00 to 2024-12-30 00:00:00\n",
            "\n",
            "Correlation Analysis with Target\n",
            "--------------------------------------------------------------------------------\n",
            "Top 10 features most correlated with Milk_Yield_L:\n",
            "Weight_kg                  0.300464\n",
            "Feed_Quantity_lb           0.223631\n",
            "Feed_Quantity_kg           0.223288\n",
            "Water_Intake_L             0.124911\n",
            "Rumination_Time_hrs        0.089823\n",
            "Previous_Week_Avg_Yield    0.089823\n",
            "Milking_Interval_hrs       0.014734\n",
            "Grazing_Duration_hrs       0.004350\n",
            "Housing_Score              0.004008\n",
            "Humidity_percent           0.002153\n",
            "Name: Milk_Yield_L, dtype: float64\n",
            "\n",
            "Bottom 10 features (least correlated or negatively correlated):\n",
            "Housing_Score            0.004008\n",
            "Humidity_percent         0.002153\n",
            "Feeding_Frequency        0.000380\n",
            "Walking_Distance_km     -0.001538\n",
            "Body_Condition_Score    -0.001647\n",
            "Resting_Hours           -0.001653\n",
            "Ambient_Temperature_C   -0.042036\n",
            "Days_in_Milk            -0.062554\n",
            "Parity                  -0.236565\n",
            "Age_Months              -0.309188\n",
            "Name: Milk_Yield_L, dtype: float64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Identify Numeric vs Categorical Features\n",
        "print(\"Feature Type Identification\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Define categorical and numeric features based on the dataset\n",
        "categorical_features = [\n",
        "    'Breed', 'Climate_Zone', 'Management_System', 'Lactation_Stage',\n",
        "    'Feed_Type', 'Mastitis'\n",
        "]\n",
        "\n",
        "# Find vaccine columns (pattern: [Disease]_Vaccine)\n",
        "vaccine_cols = [col for col in train_data.columns if '_Vaccine' in col]\n",
        "print(f\"Found {len(vaccine_cols)} vaccine columns: {vaccine_cols}\")\n",
        "categorical_features.extend(vaccine_cols)\n",
        "\n",
        "# Numeric features (excluding ID, target, and date)\n",
        "numeric_features = [\n",
        "    'Age_Months', 'Weight_kg', 'Parity', 'Days_in_Milk', 'Feed_Quantity_kg',\n",
        "    'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km',\n",
        "    'Grazing_Duration_hrs', 'Rumination_Time_hrs', 'Resting_Hours',\n",
        "    'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score',\n",
        "    'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs',\n",
        "    'Feed_Quantity_lb'\n",
        "]\n",
        "\n",
        "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
        "for feat in numeric_features:\n",
        "    print(f\"  • {feat}\")\n",
        "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
        "for feat in categorical_features:\n",
        "    print(f\"  • {feat}\")\n",
        "print()\n",
        "\n",
        "# Check for duplicate Feed_Quantity columns\n",
        "print(\"Checking Feed Quantity Columns\")\n",
        "print(\"-\" * 80)\n",
        "# Check conversion factor\n",
        "ratio = (train_data['Feed_Quantity_lb'] / train_data['Feed_Quantity_kg']).mean()\n",
        "print(f\"Average lb/kg ratio: {ratio:.4f} (expected: 2.20462)\")\n",
        "print()\n",
        "\n",
        "# Date Feature Analysis\n",
        "print(\"Date Feature Analysis\")\n",
        "print(\"-\" * 80)\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "print(f\"Date range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Correlation Analysis (for numeric features)\n",
        "print(\"Correlation Analysis with Target\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Calculate correlations with Milk_Yield_L\n",
        "correlations = train_data[numeric_features + ['Milk_Yield_L']].corr()['Milk_Yield_L'].drop('Milk_Yield_L')\n",
        "correlations = correlations.sort_values(ascending=False)\n",
        "print(\"Top 10 features most correlated with Milk_Yield_L:\")\n",
        "print(correlations.head(10))\n",
        "print()\n",
        "print(\"Bottom 10 features (least correlated or negatively correlated):\")\n",
        "print(correlations.tail(10))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Features + Outlier Checks\n",
        "\n",
        "We looked at the categorical columns to understand how many unique values each column has. This helps us later when deciding between one-hot encoding vs frequency encoding vs dropping columns that barely vary.\n",
        "\n",
        "For numeric features, outlier detection is key. We ran an IQR-based outlier detection, which gave us an idea of which features have extreme values. For every feature with outliers, we print out how many points fall outside the acceptable range → we now know where to focus during preprocessing."
      ],
      "metadata": {
        "id": "NynHh6kyKqyE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKedN30NczEd",
        "outputId": "6cafb782-d0da-4412-880d-91aa00961e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Feature Analysis\n",
            "--------------------------------------------------------------------------------\n",
            "Breed: 7 unique values\n",
            "  Values: {'Holstein': 104775, 'Jersey': 42183, 'Guernsey': 31672, 'Brown Swiss': 31155, 'Holstien': 112}\n",
            "Climate_Zone: 6 unique values\n",
            "  Values: {'Temperate': 35224, 'Tropical': 35062, 'Mediterranean': 34994, 'Arid': 34954, 'Subtropical': 34937}\n",
            "Management_System: 5 unique values\n",
            "  Values: {'Intensive': 42225, 'Pastoral': 42126, 'Extensive': 41973, 'Semi_Intensive': 41906, 'Mixed': 41770}\n",
            "Lactation_Stage: 3 unique values\n",
            "  Values: {'Mid': 83895, 'Early': 63203, 'Late': 62902}\n",
            "Feed_Type: 8 unique values\n",
            "  Values: {'Dry_Fodder': 26558, 'Pasture_Grass': 26305, 'Crop_Residues': 26278, 'Concentrates': 26231, 'Mixed_Feed': 26229}\n",
            "\n",
            "Outlier Detection Using IQR Method\n",
            "--------------------------------------------------------------------------------\n",
            "Outliers in numeric features:\n",
            "Feed_Quantity_kg                 674 outliers ( 0.32%) [bounds: 1.14 to 22.85]\n",
            "Water_Intake_L                  1485 outliers ( 0.71%) [bounds: 39.62 to 120.42]\n",
            "Walking_Distance_km              748 outliers ( 0.36%) [bounds: -1.40 to 9.40]\n",
            "Rumination_Time_hrs             2406 outliers ( 1.15%) [bounds: -17.04 to 16.70]\n",
            "Ambient_Temperature_C            196 outliers ( 0.09%) [bounds: -10.42 to 54.39]\n",
            "Previous_Week_Avg_Yield         2406 outliers ( 1.15%) [bounds: -7.94 to 24.62]\n",
            "Milking_Interval_hrs           62833 outliers (29.92%) [bounds: 12.00 to 12.00]\n",
            "Feed_Quantity_lb                 698 outliers ( 0.33%) [bounds: 2.65 to 50.27]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Categorical Feature Analysis\n",
        "print(\"Categorical Feature Analysis\")\n",
        "print(\"-\" * 80)\n",
        "for cat_feat in categorical_features[:5]:  # Show first 5\n",
        "    if cat_feat in train_data.columns:\n",
        "        n_unique = train_data[cat_feat].nunique()\n",
        "        print(f\"{cat_feat}: {n_unique} unique values\")\n",
        "        print(f\"  Values: {train_data[cat_feat].value_counts().head().to_dict()}\")\n",
        "print()\n",
        "\n",
        "# Outlier Detection\n",
        "print(\"Outlier Detection Using IQR Method\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return len(outliers), lower_bound, upper_bound\n",
        "\n",
        "print(\"Outliers in numeric features:\")\n",
        "for feature in numeric_features:\n",
        "    if feature in train_data.columns:\n",
        "        n_outliers, lower, upper = detect_outliers_iqr(train_data, feature)\n",
        "        if n_outliers > 0:\n",
        "            pct = 100 * n_outliers / len(train_data)\n",
        "            print(f\"{feature:30} {n_outliers:5} outliers ({pct:5.2f}%) [bounds: {lower:.2f} to {upper:.2f}]\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "\n",
        "We first removed the redundant column of Feed_Quantity_lb, since it represents the same information as Feed_Quantity_kg, except in pounds. We then processed the Date column (it was converted to datetime during data exploration). We extract the month, dayOfWeek, and quarter from the date. We also add cyclical month features (sin / cos — to transform the month number into two new features). This is because seasons loop around, and models don't naturally understand that December is next to January.\n",
        "\n",
        "For the missing values, we filled in the numeric columns using the training means. For outliers, instead of deleting the rows, we capped them based on 3*IQR. We did this for all columns with outliers except Milking_Interval_hrs: we skipped this column because it had an unusual distribution.\n",
        "\n",
        "Next, we handled the categorical variables. For binary features, we used simple label encoding, and for other low-cardinality features, we applied one-hot encoding. Importantly, we always fit the encodings on the training set first and then aligned the test set to match those columns.\n",
        "\n",
        "Finally, we removed low-variance features (columns where 95%+ of values were the same) because they don't help the model. We also ran a safety check at the very end to make sure there were no more missing values in the data (for test and train).\n",
        "\n",
        "The result of this data cleaning cell is two fully cleaned dataframes that are safe to feed into a model without worrying about missing data, redundant/useless features, or inconsistent preprocessing."
      ],
      "metadata": {
        "id": "fkkivTNBOlyL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvoyzqwFc_DI",
        "outputId": "b5dd4f34-a25a-4152-c5bc-0157ad3d6007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PHASE 2: DATA CLEANING\n",
            "================================================================================\n",
            "\n",
            "Cleaning training and test data (no leakage)...\n",
            "--------------------------------------------------------------------------------\n",
            "Removing Redundant Features\n",
            "----------------------------------------\n",
            "\n",
            "Processing Date Feature\n",
            "----------------------------------------\n",
            "✓ (training) Extracted: Month, DayOfWeek, Quarter\n",
            "✓ (training) Created cyclical features: Month_sin, Month_cos\n",
            "✓ (training) Dropped original Date column\n",
            "✓ (test) Extracted: Month, DayOfWeek, Quarter\n",
            "✓ (test) Created cyclical features: Month_sin, Month_cos\n",
            "✓ (test) Dropped original Date column\n",
            "\n",
            "Handling Missing Values (Numeric)\n",
            "----------------------------------------\n",
            "  (train) Feed_Quantity_kg                4.99% missing → imputed with median (12.01)\n",
            "  (test)  Feed_Quantity_kg               imputed with train median (12.01)\n",
            "  (train) Housing_Score                   2.99% missing → imputed with median (0.65)\n",
            "  (test)  Housing_Score                  imputed with train median (0.65)\n",
            "\n",
            "Handling Outliers (Capping Method)\n",
            "----------------------------------------\n",
            "  (train) Rumination_Time_hrs            capped 0 lower, 2 upper outliers\n",
            "  (train) Previous_Week_Avg_Yield        capped 0 lower, 2 upper outliers\n",
            "  (train/test) Milking_Interval_hrs           skipped outlier capping\n",
            "\n",
            "Encoding Categorical Variables\n",
            "----------------------------------------\n",
            "  (train) Breed                          7 categories → One-hot encoded\n",
            "  (test)  Breed                          aligned one-hot columns to train\n",
            "  (train) Climate_Zone                   6 categories → One-hot encoded\n",
            "  (test)  Climate_Zone                   aligned one-hot columns to train\n",
            "  (train) Management_System              5 categories → One-hot encoded\n",
            "  (test)  Management_System              aligned one-hot columns to train\n",
            "  (train) Lactation_Stage                3 categories → One-hot encoded\n",
            "  (test)  Lactation_Stage                aligned one-hot columns to train\n",
            "  (train) Feed_Type                      8 categories → One-hot encoded\n",
            "  (test)  Feed_Type                      aligned one-hot columns to train\n",
            "  (train) Mastitis                       Binary → Label encoded\n",
            "  (test)  Mastitis                       Binary → Label encoded with train mapping\n",
            "  (train) FMD_Vaccine                    Binary → Label encoded\n",
            "  (test)  FMD_Vaccine                    Binary → Label encoded with train mapping\n",
            "  (train) Brucellosis_Vaccine            Binary → Label encoded\n",
            "  (test)  Brucellosis_Vaccine            Binary → Label encoded with train mapping\n",
            "  (train) HS_Vaccine                     Binary → Label encoded\n",
            "  (test)  HS_Vaccine                     Binary → Label encoded with train mapping\n",
            "  (train) BQ_Vaccine                     Binary → Label encoded\n",
            "  (test)  BQ_Vaccine                     Binary → Label encoded with train mapping\n",
            "  (train) Anthrax_Vaccine                Binary → Label encoded\n",
            "  (test)  Anthrax_Vaccine                Binary → Label encoded with train mapping\n",
            "  (train) IBR_Vaccine                    Binary → Label encoded\n",
            "  (test)  IBR_Vaccine                    Binary → Label encoded with train mapping\n",
            "  (train) BVD_Vaccine                    Binary → Label encoded\n",
            "  (test)  BVD_Vaccine                    Binary → Label encoded with train mapping\n",
            "  (train) Rabies_Vaccine                 Binary → Label encoded\n",
            "  (test)  Rabies_Vaccine                 Binary → Label encoded with train mapping\n",
            "\n",
            "Removing Low-Variance Features\n",
            "----------------------------------------\n",
            "  No low-variance features detected\n",
            "\n",
            "Final Safety Check for NaN Values\n",
            "----------------------------------------\n",
            "  (training) ✓ No NaN values detected\n",
            "  (test) ✓ No NaN values detected\n",
            "\n",
            "✓ Training data shape after cleaning: (210000, 58)\n",
            "✓ Test data shape after cleaning: (40000, 57)\n",
            "\n",
            "Columns — train (58): ['Cattle_ID', 'Age_Months', 'Weight_kg', 'Parity', 'Days_in_Milk', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Rumination_Time_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Farm_ID', 'Mastitis', 'Milk_Yield_L', 'Month', 'DayOfWeek', 'Quarter', 'Month_sin', 'Month_cos', 'Breed_Brown Swiss', 'Breed_Brown Swiss ', 'Breed_Guernsey', 'Breed_Holstein', 'Breed_Holstien', 'Breed_Jersey', 'Climate_Zone_Continental', 'Climate_Zone_Mediterranean', 'Climate_Zone_Subtropical', 'Climate_Zone_Temperate', 'Climate_Zone_Tropical', 'Management_System_Intensive', 'Management_System_Mixed', 'Management_System_Pastoral', 'Management_System_Semi_Intensive', 'Lactation_Stage_Late', 'Lactation_Stage_Mid', 'Feed_Type_Crop_Residues', 'Feed_Type_Dry_Fodder', 'Feed_Type_Green_Fodder', 'Feed_Type_Hay', 'Feed_Type_Mixed_Feed', 'Feed_Type_Pasture_Grass', 'Feed_Type_Silage']\n",
            "Columns — test (57): ['Cattle_ID', 'Age_Months', 'Weight_kg', 'Parity', 'Days_in_Milk', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Rumination_Time_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Farm_ID', 'Mastitis', 'Month', 'DayOfWeek', 'Quarter', 'Month_sin', 'Month_cos', 'Breed_Brown Swiss', 'Breed_Brown Swiss ', 'Breed_Guernsey', 'Breed_Holstein', 'Breed_Holstien', 'Breed_Jersey', 'Climate_Zone_Continental', 'Climate_Zone_Mediterranean', 'Climate_Zone_Subtropical', 'Climate_Zone_Temperate', 'Climate_Zone_Tropical', 'Management_System_Intensive', 'Management_System_Mixed', 'Management_System_Pastoral', 'Management_System_Semi_Intensive', 'Lactation_Stage_Late', 'Lactation_Stage_Mid', 'Feed_Type_Crop_Residues', 'Feed_Type_Dry_Fodder', 'Feed_Type_Green_Fodder', 'Feed_Type_Hay', 'Feed_Type_Mixed_Feed', 'Feed_Type_Pasture_Grass', 'Feed_Type_Silage']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 2: DATA CLEANING\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 2: DATA CLEANING\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "def clean_data(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Comprehensive data cleaning function for cattle milk yield dataset.\n",
        "\n",
        "    Args:\n",
        "        train_df: Training DataFrame (has target)\n",
        "        test_df:  Test DataFrame (no target)\n",
        "\n",
        "    Returns:\n",
        "        (train_clean, test_clean): cleaned DataFrames\n",
        "    \"\"\"\n",
        "    train_clean = train_df.copy()\n",
        "    test_clean = test_df.copy()\n",
        "\n",
        "    print(\"Cleaning training and test data (no leakage)...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Drop redundant Feed_Quantity_lb (duplicate of Feed_Quantity_kg)\n",
        "    print(\"Removing Redundant Features\")\n",
        "    print(\"-\" * 40)\n",
        "    for df_name, df_clean in [(\"training\", train_clean), (\"test\", test_clean)]:\n",
        "          df_clean.drop('Feed_Quantity_lb', axis=1, inplace=True)\n",
        "    print()\n",
        "\n",
        "    # Handle Date Feature - Extract temporal features\n",
        "    print(\"Processing Date Feature\")\n",
        "    print(\"-\" * 40)\n",
        "    for df_name, df_clean in [(\"training\", train_clean), (\"test\", test_clean)]:\n",
        "        if 'Date' in df_clean.columns:\n",
        "            df_clean['Date'] = pd.to_datetime(df_clean['Date'])\n",
        "\n",
        "            # Extract useful temporal features\n",
        "            df_clean['Month'] = df_clean['Date'].dt.month\n",
        "            df_clean['DayOfWeek'] = df_clean['Date'].dt.dayofweek\n",
        "            df_clean['Quarter'] = df_clean['Date'].dt.quarter\n",
        "\n",
        "            # Create cyclical features for month (since December is close to January)\n",
        "            df_clean['Month_sin'] = np.sin(2 * np.pi * df_clean['Month'] / 12)\n",
        "            df_clean['Month_cos'] = np.cos(2 * np.pi * df_clean['Month'] / 12)\n",
        "\n",
        "            print(f\"✓ ({df_name}) Extracted: Month, DayOfWeek, Quarter\")\n",
        "            print(f\"✓ ({df_name}) Created cyclical features: Month_sin, Month_cos\")\n",
        "\n",
        "            # Drop original Date column (not useful for ML models directly)\n",
        "            df_clean.drop('Date', axis=1, inplace=True)\n",
        "            print(f\"✓ ({df_name}) Dropped original Date column\")\n",
        "    print()\n",
        "\n",
        "    # Handle Missing Values (NUMERIC) - FIT ON TRAIN, APPLY TO TEST\n",
        "    print(\"Handling Missing Values (Numeric)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    numeric_means = {}  # store means from training only\n",
        "\n",
        "    for col in numeric_features:\n",
        "        if col in train_clean.columns:\n",
        "            missing_pct = 100 * train_clean[col].isnull().sum() / len(train_clean)\n",
        "            if train_clean[col].isnull().sum() > 0:\n",
        "                mean_val = train_clean[col].mean()\n",
        "                numeric_means[col] = mean_val\n",
        "                train_clean[col].fillna(mean_val, inplace=True)\n",
        "                print(f\"  (train) {col:30} {missing_pct:5.2f}% missing → imputed with median ({mean_val:.2f})\")\n",
        "\n",
        "                if col in test_clean.columns:\n",
        "                    test_clean[col].fillna(mean_val, inplace=True)\n",
        "                    print(f\"  (test)  {col:30} imputed with train median ({mean_val:.2f})\")\n",
        "            else:\n",
        "                # still store for consistency\n",
        "                numeric_means[col] = train_clean[col].median()\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Handle Outliers (Cap rather than remove) - FIT BOUNDS ON TRAIN, APPLY TO TEST\n",
        "    print(\"Handling Outliers (Capping Method)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    outlier_count = 0\n",
        "    for col in numeric_features:\n",
        "        if col in train_clean.columns:\n",
        "            if col == 'Milking_Interval_hrs':\n",
        "                print(f\"  (train/test) {col:30} skipped outlier capping\")\n",
        "                continue\n",
        "\n",
        "            Q1 = train_clean[col].quantile(0.25)\n",
        "            Q3 = train_clean[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 3 * IQR  # Use 3*IQR for conservative capping\n",
        "            upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "            n_lower_train = (train_clean[col] < lower_bound).sum()\n",
        "            n_upper_train = (train_clean[col] > upper_bound).sum()\n",
        "\n",
        "            if n_lower_train > 0 or n_upper_train > 0:\n",
        "                train_clean[col] = train_clean[col].clip(lower_bound, upper_bound)\n",
        "                outlier_count += 1\n",
        "                print(f\"  (train) {col:30} capped {n_lower_train} lower, {n_upper_train} upper outliers\")\n",
        "\n",
        "            # apply same bounds to test (if column exists)\n",
        "            if col in test_clean.columns:\n",
        "                test_clean[col] = test_clean[col].clip(lower_bound, upper_bound)\n",
        "\n",
        "    if outlier_count == 0:\n",
        "        print(\"  No significant outliers detected (using 3*IQR threshold)\")\n",
        "    print()\n",
        "\n",
        "    # Encode Categorical Variables - FIT ON TRAIN, APPLY TO TEST\n",
        "    print(\"Encoding Categorical Variables\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for col in categorical_features:\n",
        "        if col in train_clean.columns:\n",
        "            n_unique = train_clean[col].nunique()\n",
        "\n",
        "            # Binary: Simple label encoding (0/1) – fit on train\n",
        "            if n_unique <= 2:\n",
        "                le = LabelEncoder()\n",
        "                train_clean[col] = train_clean[col].astype(str)\n",
        "                le.fit(train_clean[col])\n",
        "                mapping = {cls: idx for idx, cls in enumerate(le.classes_)}\n",
        "\n",
        "                train_clean[col] = train_clean[col].map(mapping).astype(int)\n",
        "                print(f\"  (train) {col:30} Binary → Label encoded\")\n",
        "\n",
        "                # Apply same mapping to test; unseen categories → -1\n",
        "                if col in test_clean.columns:\n",
        "                    test_clean[col] = test_clean[col].astype(str).map(mapping).fillna(-1).astype(int)\n",
        "                    print(f\"  (test)  {col:30} Binary → Label encoded with train mapping\")\n",
        "\n",
        "            # Low cardinality: One-hot encoding – fit categories on train\n",
        "            elif n_unique < 10:\n",
        "                # TRAIN dummies\n",
        "                dummies_train = pd.get_dummies(train_clean[col], prefix=col, drop_first=True)\n",
        "                train_clean = pd.concat([train_clean, dummies_train], axis=1)\n",
        "                train_clean.drop(col, axis=1, inplace=True)\n",
        "\n",
        "                print(f\"  (train) {col:30} {n_unique} categories → One-hot encoded\")\n",
        "\n",
        "                # TEST dummies with same columns\n",
        "                if col in test_clean.columns:\n",
        "                    dummies_test = pd.get_dummies(test_clean[col], prefix=col, drop_first=True)\n",
        "                    # Align to training dummy columns\n",
        "                    dummies_test = dummies_test.reindex(columns=dummies_train.columns, fill_value=0)\n",
        "                    test_clean = pd.concat([test_clean, dummies_test], axis=1)\n",
        "                    test_clean.drop(col, axis=1, inplace=True)\n",
        "                    print(f\"  (test)  {col:30} aligned one-hot columns to train\")\n",
        "    print()\n",
        "\n",
        "    # Remove Low-Variance Features – DECIDE ON TRAIN, DROP IN BOTH\n",
        "    print(\"Removing Low-Variance Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    removed_features = []\n",
        "    exclude_cols = ['Cattle_ID', 'Farm_ID', 'Milk_Yield_L']\n",
        "\n",
        "    for col in train_clean.select_dtypes(include=[np.number]).columns:\n",
        "        if col not in exclude_cols:\n",
        "            if len(train_clean[col].value_counts()) > 0:\n",
        "                mode_freq = train_clean[col].value_counts().iloc[0] / len(train_clean)\n",
        "                if mode_freq > 0.95:\n",
        "                    removed_features.append(col)\n",
        "\n",
        "    # Drop same features from both train and test\n",
        "    if removed_features:\n",
        "        train_clean.drop(columns=removed_features, axis=1, inplace=True, errors='ignore')\n",
        "        test_clean.drop(columns=removed_features, axis=1, inplace=True, errors='ignore')\n",
        "        print(f\"  Removed {len(removed_features)} low-variance features (from train, applied to test too):\")\n",
        "        for feat in removed_features:\n",
        "            print(f\"    • {feat}\")\n",
        "    else:\n",
        "        print(\"  No low-variance features detected\")\n",
        "    print()\n",
        "\n",
        "    # Final safety check - fill any remaining NaN values\n",
        "    print(\"Final Safety Check for NaN Values\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Do this for both train and test, but use train medians where possible\n",
        "    for df_name, df_clean in [(\"training\", train_clean), (\"test\", test_clean)]:\n",
        "        remaining_nan = df_clean.isnull().sum().sum()\n",
        "        if remaining_nan > 0:\n",
        "            print(f\"  ({df_name}) WARNING: Found {remaining_nan} remaining NaN values\")\n",
        "            print(f\"  ({df_name}) → Filling remaining numeric NaN with TRAIN medians / 0\")\n",
        "\n",
        "            # For numeric columns, fill with medians based on training data\n",
        "            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_cols:\n",
        "                if df_clean[col].isnull().any():\n",
        "                    if col in train_clean.columns:\n",
        "                        # median from train distribution\n",
        "                        median_val = train_clean[col].median()\n",
        "                    else:\n",
        "                        # column not in train (e.g., test-only) – just use 0\n",
        "                        median_val = 0\n",
        "                    df_clean[col].fillna(median_val, inplace=True)\n",
        "\n",
        "            # For any remaining non-numeric NaN, fill with 'Unknown'\n",
        "            non_num_cols = df_clean.select_dtypes(exclude=[np.number]).columns\n",
        "            df_clean[non_num_cols] = df_clean[non_num_cols].fillna('Unknown')\n",
        "        else:\n",
        "            print(f\"  ({df_name}) ✓ No NaN values detected\")\n",
        "\n",
        "        # Check for infinite values in each df separately\n",
        "        inf_count = np.isinf(df_clean.select_dtypes(include=[np.number]).values).sum()\n",
        "        if inf_count > 0:\n",
        "            print(f\"  ({df_name}) WARNING: Found {inf_count} infinite values → Replacing with finite caps\")\n",
        "            df_clean.replace([np.inf, -np.inf], [1e10, -1e10], inplace=True)\n",
        "\n",
        "    print()\n",
        "    return train_clean, test_clean\n",
        "\n",
        "# Apply cleaning to both train and test\n",
        "train_cleaned, test_cleaned = clean_data(train_data, test_data)\n",
        "\n",
        "print(f\"✓ Training data shape after cleaning: {train_cleaned.shape}\")\n",
        "print(f\"✓ Test data shape after cleaning: {test_cleaned.shape}\")\n",
        "print()\n",
        "\n",
        "print(\n",
        "    f\"Columns — train ({len(train_cleaned.columns)}): {list(train_cleaned.columns)}\\n\"\n",
        "    f\"Columns — test ({len(test_cleaned.columns)}): {list(test_cleaned.columns)}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "ujIr-16NYxoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is clean, we started adding features that improve the model's accuracy by adding in some domain intuition about dairy cows and milk production.\n",
        "\n",
        "First, we created ratio features for productivity and efficiency, like feed per unit weight, water per unit weight, and milk yield per kg of feed. These ratio features are meant to capture how efficiently each cow converts its feed and water into milk, rather than just using the raw totals.\n",
        "\n",
        "Next, we added in activity and health-related features using walking distance, grazing duration, rumination time, resting hours, body condition score, etc. More balanced activity and rest, plus better body condition = healthier, more productive cows.\n",
        "\n",
        "We also focused on lactation-specific features. We added these features: whether or not the cow is in its “peak” lactation window (cows produce the most milk at around 60 days), how often it's being milked per day based on the milking interval, and parity-related information (first-time vs experienced mothers — milk production changes with each lactation).\n",
        "\n",
        "For environment and age, we created simple indicators for extreme temperatures (based on the Ambient_Temperature_C column), plus age groups (young, prime, senior) and an approximate average age at calving. These features are meant to capture long-term effects like aging and environmental stress on milk yield.\n",
        "\n",
        "We also added some feed and nutrition features like feed per meal and feed-to-water ratio, to reflect how feeding routines might affect production. After this, we scanned our data for highly skewed distributions and then applied log transforms on them (to stabilize).\n",
        "\n",
        "Finally, we included some interaction features (like weight × age, feed × water, parity × days in milk). These features let the model pick up on some important combinations of these features that might not be so obvious from the raw features alone.\n",
        "\n",
        "All of this is done on the test and train data.\n",
        "\n",
        "Links:\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S2666910225000924#:~:text=In%20conclusion%2C%20cow%20lactation%20milk,linearly%20thereafter%20with%20parity%20number.\n",
        "\n",
        "https://extension.psu.edu/evaluating-milk-peak-and-persistency-using-dhia-data-part-2#:~:text=Cows%20usually%20peak%20around%2045,milk%20over%20her%20entire%20lactation.\n"
      ],
      "metadata": {
        "id": "_C95wIfXa9kE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGn4CwdkdGTa",
        "outputId": "a5e9079d-1653-4420-806c-c3733b53468f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Engineering features for training data...\n",
            "--------------------------------------------------------------------------------\n",
            "Creating Productivity & Efficiency Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Weight (feed efficiency)\n",
            "  ✓ Created: Water_Per_Weight\n",
            "  ✓ Created: Yield_Per_Feed (production efficiency)\n",
            "\n",
            "Creating Activity & Health Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Activity_Level\n",
            "  ✓ Created: Rest_Rumination_Ratio\n",
            "  ✓ Created: Adjusted_BCS (body condition adjusted for weight)\n",
            "\n",
            "Creating Lactation-Specific Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Milkings_Per_Day\n",
            "  ✓ Created: First_Time_Mother, Experienced_Mother\n",
            "\n",
            "Creating Environmental Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Heat_Stress, Cold_Stress indicators\n",
            "\n",
            "Creating Age & Experience Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Age_Years, Young_Cow, Prime_Age, Senior_Cow\n",
            "  ✓ Created: Avg_Age_At_Calving\n",
            "\n",
            "Creating Feed & Nutrition Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Meal\n",
            "  ✓ Created: Feed_Water_Ratio\n",
            "\n",
            "Applying Log Transformations to Skewed Features\n",
            "----------------------------------------\n",
            "  Applied log transformation to 8 skewed features:\n",
            "    • Milking_Interval_hrs (skewness: 1.71)\n",
            "    • Mastitis (skewness: 2.67)\n",
            "    • Yield_Per_Feed (skewness: 1.54)\n",
            "    • Rest_Rumination_Ratio (skewness: -5.22)\n",
            "    • Peak_Lactation (skewness: 2.43)\n",
            "\n",
            "Creating Advanced Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Weight_Age_Interaction\n",
            "  ✓ Created: Feed_Water_Interaction\n",
            "  ✓ Created: Parity_DIM_Interaction\n",
            "\n",
            "Engineering features for test data...\n",
            "--------------------------------------------------------------------------------\n",
            "Creating Productivity & Efficiency Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Weight (feed efficiency)\n",
            "  ✓ Created: Water_Per_Weight\n",
            "  ✓ Created: Yield_Per_Feed (production efficiency)\n",
            "\n",
            "Creating Activity & Health Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Activity_Level\n",
            "  ✓ Created: Rest_Rumination_Ratio\n",
            "  ✓ Created: Adjusted_BCS (body condition adjusted for weight)\n",
            "\n",
            "Creating Lactation-Specific Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Milkings_Per_Day\n",
            "  ✓ Created: First_Time_Mother, Experienced_Mother\n",
            "\n",
            "Creating Environmental Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Heat_Stress, Cold_Stress indicators\n",
            "\n",
            "Creating Age & Experience Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Age_Years, Young_Cow, Prime_Age, Senior_Cow\n",
            "  ✓ Created: Avg_Age_At_Calving\n",
            "\n",
            "Creating Feed & Nutrition Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Meal\n",
            "  ✓ Created: Feed_Water_Ratio\n",
            "\n",
            "Applying Log Transformations to Skewed Features\n",
            "----------------------------------------\n",
            "  Applied log transformation to 8 skewed features:\n",
            "    • Milking_Interval_hrs (skewness: 1.70)\n",
            "    • Mastitis (skewness: 2.71)\n",
            "    • Yield_Per_Feed (skewness: 1.52)\n",
            "    • Rest_Rumination_Ratio (skewness: 5.86)\n",
            "    • Peak_Lactation (skewness: 2.44)\n",
            "\n",
            "Creating Advanced Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Weight_Age_Interaction\n",
            "  ✓ Created: Feed_Water_Interaction\n",
            "  ✓ Created: Parity_DIM_Interaction\n",
            "\n",
            "✓ Training data shape after feature engineering: (210000, 88)\n",
            "✓ Test data shape after feature engineering: (40000, 87)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def engineer_features(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Create domain-specific features for dairy cow milk yield prediction.\n",
        "\n",
        "    Args:\n",
        "        df: Cleaned DataFrame\n",
        "        is_train: Boolean indicating if this is training data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with engineered features\n",
        "    \"\"\"\n",
        "    df_eng = df.copy()\n",
        "\n",
        "    print(f\"Engineering features for {'training' if is_train else 'test'} data...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Productivity & Efficiency Ratios\n",
        "    print(\"Creating Productivity & Efficiency Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if 'Feed_Quantity_kg' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "        # Feed efficiency: feed per kg of body weight\n",
        "        df_eng['Feed_Per_Weight'] = df_eng['Feed_Quantity_kg'] / (df_eng['Weight_kg'] + 1)\n",
        "        print(\"  ✓ Created: Feed_Per_Weight (feed efficiency)\")\n",
        "\n",
        "    if 'Water_Intake_L' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "        # Water consumption per kg\n",
        "        df_eng['Water_Per_Weight'] = df_eng['Water_Intake_L'] / (df_eng['Weight_kg'] + 1)\n",
        "        print(\"  ✓ Created: Water_Per_Weight\")\n",
        "\n",
        "    if 'Previous_Week_Avg_Yield' in df_eng.columns and 'Feed_Quantity_kg' in df_eng.columns:\n",
        "        # Yield efficiency: milk per kg of feed\n",
        "        df_eng['Yield_Per_Feed'] = df_eng['Previous_Week_Avg_Yield'] / (df_eng['Feed_Quantity_kg'] + 1)\n",
        "        print(\"  ✓ Created: Yield_Per_Feed (production efficiency)\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Activity & Health Indicators\n",
        "    print(\"Creating Activity & Health Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Walking_Distance_km', 'Grazing_Duration_hrs']):\n",
        "        # Activity level\n",
        "        df_eng['Activity_Level'] = df_eng['Walking_Distance_km'] + df_eng['Grazing_Duration_hrs']\n",
        "        print(\"  ✓ Created: Activity_Level\")\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Rumination_Time_hrs', 'Resting_Hours']):\n",
        "        # Rest/Rumination balance\n",
        "        df_eng['Rest_Rumination_Ratio'] = df_eng['Resting_Hours'] / (df_eng['Rumination_Time_hrs'] + 1)\n",
        "        print(\"  ✓ Created: Rest_Rumination_Ratio\")\n",
        "\n",
        "    if 'Body_Condition_Score' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "        # Adjusted body condition\n",
        "        df_eng['Adjusted_BCS'] = df_eng['Body_Condition_Score'] * df_eng['Weight_kg'] / 100\n",
        "        print(\"  ✓ Created: Adjusted_BCS (body condition adjusted for weight)\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Lactation-Related Features\n",
        "    print(\"Creating Lactation-Specific Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if 'Days_in_Milk' in df_eng.columns and 'Milking_Interval_hrs' in df_eng.columns:\n",
        "        # Milkings per day\n",
        "        df_eng['Milkings_Per_Day'] = 24 / (df_eng['Milking_Interval_hrs'] + 1)\n",
        "        print(\"  ✓ Created: Milkings_Per_Day\")\n",
        "\n",
        "    if 'Days_in_Milk' in df_eng.columns:\n",
        "        # Lactation curve features (peak milk around 60 days)\n",
        "        # df_eng['DIM_Squared'] = df_eng['Days_in_Milk'] ** 2\n",
        "        df_eng['Peak_Lactation'] = (df_eng['Days_in_Milk'] >= 40) & (df_eng['Days_in_Milk'] <= 80)\n",
        "        df_eng['Peak_Lactation'] = df_eng['Peak_Lactation'].astype(int)\n",
        "        # print(\"  ✓ Created: DIM_Squared, Peak_Lactation (lactation curve)\")\n",
        "\n",
        "    if 'Parity' in df_eng.columns:\n",
        "        # Parity groups (first-time vs experienced)\n",
        "        df_eng['First_Time_Mother'] = (df_eng['Parity'] == 0).astype(int)\n",
        "        df_eng['Experienced_Mother'] = (df_eng['Parity'] >= 2).astype(int)\n",
        "        print(\"  ✓ Created: First_Time_Mother, Experienced_Mother\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Environmental Interactions\n",
        "    print(\"Creating Environmental Interaction Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if 'Ambient_Temperature_C' in df_eng.columns:\n",
        "        # Temperature stress indicators\n",
        "        df_eng['Heat_Stress'] = (df_eng['Ambient_Temperature_C'] > 25).astype(int)\n",
        "        df_eng['Cold_Stress'] = (df_eng['Ambient_Temperature_C'] < 5).astype(int)\n",
        "        print(\"  ✓ Created: Heat_Stress, Cold_Stress indicators\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Age & Experience Features\n",
        "    print(\"Creating Age & Experience Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if 'Age_Months' in df_eng.columns:\n",
        "        # Age groups\n",
        "        df_eng['Age_Years'] = df_eng['Age_Months'] / 12\n",
        "        df_eng['Young_Cow'] = (df_eng['Age_Months'] < 30).astype(int)  # Less than 2.5 years\n",
        "        df_eng['Prime_Age'] = ((df_eng['Age_Months'] >= 30) & (df_eng['Age_Months'] <= 72)).astype(int)\n",
        "        df_eng['Senior_Cow'] = (df_eng['Age_Months'] > 72).astype(int)  # Over 6 years\n",
        "        print(\"  ✓ Created: Age_Years, Young_Cow, Prime_Age, Senior_Cow\")\n",
        "\n",
        "    if 'Age_Months' in df_eng.columns and 'Parity' in df_eng.columns:\n",
        "        # Average age at calving\n",
        "        df_eng['Avg_Age_At_Calving'] = df_eng['Age_Months'] / (df_eng['Parity'] + 1)\n",
        "        print(\"  ✓ Created: Avg_Age_At_Calving\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Feed & Nutrition Features\n",
        "    print(\"Creating Feed & Nutrition Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Feed_Quantity_kg', 'Feeding_Frequency']):\n",
        "        # Feed per meal\n",
        "        df_eng['Feed_Per_Meal'] = df_eng['Feed_Quantity_kg'] / (df_eng['Feeding_Frequency'] + 1)\n",
        "        print(\"  ✓ Created: Feed_Per_Meal\")\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Feed_Quantity_kg', 'Water_Intake_L']):\n",
        "        # Feed to water ratio\n",
        "        df_eng['Feed_Water_Ratio'] = df_eng['Feed_Quantity_kg'] / (df_eng['Water_Intake_L'] + 1)\n",
        "        print(\"  ✓ Created: Feed_Water_Ratio\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Log Transformations for Skewed Features\n",
        "    print(\"Applying Log Transformations to Skewed Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Identify highly skewed features\n",
        "    skewed_features = []\n",
        "    for col in df_eng.select_dtypes(include=[np.number]).columns:\n",
        "        if col not in ['Cattle_ID', 'Farm_ID', 'Milk_Yield_L']:\n",
        "            skewness = df_eng[col].skew()\n",
        "            if abs(skewness) > 1.5:  # Threshold for high skewness\n",
        "                skewed_features.append((col, skewness))\n",
        "                df_eng[f'{col}_log'] = np.log1p(df_eng[col])  # log1p handles zeros\n",
        "\n",
        "    if skewed_features:\n",
        "        print(f\"  Applied log transformation to {len(skewed_features)} skewed features:\")\n",
        "        for feat, skew in skewed_features[:5]:  # Show first 5\n",
        "            print(f\"    • {feat} (skewness: {skew:.2f})\")\n",
        "    else:\n",
        "        print(\"  No highly skewed features detected\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Advanced Interaction & Polynomial Features\n",
        "    print(\"Creating Advanced Interaction Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # KEY INTERACTION: Weight × Age (older heavier cows produce differently)\n",
        "    if 'Weight_kg' in df_eng.columns and 'Age_Months' in df_eng.columns:\n",
        "        df_eng['Weight_Age_Interaction'] = (df_eng['Weight_kg'] * df_eng['Age_Months']) / 1000\n",
        "        print(\"  ✓ Created: Weight_Age_Interaction\")\n",
        "\n",
        "    # Feed × Water interaction (nutritional synergy)\n",
        "    if 'Feed_Quantity_kg' in df_eng.columns and 'Water_Intake_L' in df_eng.columns:\n",
        "        df_eng['Feed_Water_Interaction'] = df_eng['Feed_Quantity_kg'] * df_eng['Water_Intake_L']\n",
        "        print(\"  ✓ Created: Feed_Water_Interaction\")\n",
        "\n",
        "    # Parity × Days in Milk (experienced cows at different lactation stages)\n",
        "    if 'Parity' in df_eng.columns and 'Days_in_Milk' in df_eng.columns:\n",
        "        df_eng['Parity_DIM_Interaction'] = df_eng['Parity'] * df_eng['Days_in_Milk']\n",
        "        print(\"  ✓ Created: Parity_DIM_Interaction\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    return df_eng\n",
        "\n",
        "# Apply feature engineering\n",
        "train_featured = engineer_features(train_cleaned, is_train=True)\n",
        "test_featured = engineer_features(test_cleaned, is_train=False)\n",
        "\n",
        "print(f\"✓ Training data shape after feature engineering: {train_featured.shape}\")\n",
        "print(f\"✓ Test data shape after feature engineering: {test_featured.shape}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Feature Engineering Ideas We Tried\n",
        "\n",
        "\n",
        "\n",
        "*   Heat_Stress_Index: We tried a temperature–humidity interaction feature, but it didn't noticeably improve performance. It just added another formula on top of existing temp and humidity features. Since the simple heat_stress and cold_stress feats were capturing the main effect, this feature was not necessary.\n",
        "*   Checks for NaN inside engineer_features: The data cleaning phase already handles missing and invalid values. Adding another round of checks here is redundant.\n",
        "\n",
        "*   Polynomial and curve-based features (squared terms, lactation peak, Log_DIM): These feats were trying to capture non-linear relationships more explicitly. However, these features did not noticeably improve performance. Instead, they just increased the dimensionality of the data and risked overfitting. Thus, we decided to focus and prioritize the more domain-driven features.\n",
        "*   Specific interaction features (Holstein_Weight): Again, we didn't notice an improvement in performance, so we dropped them in favor of more general interaction terms that are less brittle.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A-zqixrhlfoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if all(col in df_eng.columns for col in ['Ambient_Temperature_C', 'Humidity_percent']):\n",
        "    #     # Heat stress index (simplified THI - Temperature Humidity Index)\n",
        "    #     # THI = T - 0.55 * (1 - RH/100) * (T - 58) simplified version\n",
        "    #     df_eng['Heat_Stress_Index'] = (df_eng['Ambient_Temperature_C'] +\n",
        "    #                                    0.36 * df_eng['Humidity_percent'] / 100 *\n",
        "    #                                    df_eng['Ambient_Temperature_C'])\n",
        "    #     print(\"  ✓ Created: Heat_Stress_Index (temperature-humidity interaction)\")\n",
        "\n",
        "\n",
        "# Final check for NaN/Inf in engineered features\n",
        "    # print(\"3.8 Final Check for Invalid Values in Engineered Features\")\n",
        "    # print(\"-\" * 40)\n",
        "\n",
        "    # # Replace any inf values created during feature engineering\n",
        "    # inf_count = np.isinf(df_eng.select_dtypes(include=[np.number]).values).sum()\n",
        "    # if inf_count > 0:\n",
        "    #     print(f\"  Found {inf_count} infinite values in engineered features\")\n",
        "    #     df_eng = df_eng.replace([np.inf, -np.inf], [1e10, -1e10])\n",
        "    #     print(f\"  ✓ Replaced infinite values\")\n",
        "\n",
        "    # # Fill any NaN created during feature engineering\n",
        "    # nan_count = df_eng.isnull().sum().sum()\n",
        "    # if nan_count > 0:\n",
        "    #     print(f\"  Found {nan_count} NaN values in engineered features\")\n",
        "    #     numeric_cols = df_eng.select_dtypes(include=[np.number]).columns\n",
        "    #     df_eng[numeric_cols] = df_eng[numeric_cols].fillna(df_eng[numeric_cols].median())\n",
        "    #     print(f\"  ✓ Filled NaN values with median\")\n",
        "    # else:\n",
        "    #     print(f\"  ✓ No invalid values in engineered features\")\n",
        "\n",
        "    # print()\n",
        "\n",
        "# POLYNOMIAL FEATURES (capture non-linear relationships)\n",
        "    # if 'Weight_kg' in df_eng.columns:\n",
        "    #     df_eng['Weight_Squared'] = df_eng['Weight_kg'] ** 2\n",
        "    #     print(\"  ✓ Created: Weight_Squared\")\n",
        "\n",
        "    # if 'Previous_Week_Avg_Yield' in df_eng.columns:\n",
        "    #     df_eng['Previous_Week_Yield_Squared'] = df_eng['Previous_Week_Avg_Yield'] ** 2\n",
        "    #     print(\"  ✓ Created: Previous_Week_Yield_Squared\")\n",
        "\n",
        "    # # Better lactation curve (Wood's curve approximation)\n",
        "    # if 'Days_in_Milk' in df_eng.columns:\n",
        "    #     df_eng['Lactation_Peak'] = df_eng['Days_in_Milk'] * np.exp(-0.05 * df_eng['Days_in_Milk'])\n",
        "    #     df_eng['Log_DIM'] = np.log1p(df_eng['Days_in_Milk'])\n",
        "    #     print(\"  ✓ Created: Lactation_Peak, Log_DIM (improved lactation curve)\")\n",
        "\n",
        "    # # Breed-specific features (Holstein is highest producer)\n",
        "    # if 'Breed_Holstein' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "    #     df_eng['Holstein_Weight'] = df_eng['Breed_Holstein'] * df_eng['Weight_kg']\n",
        "    #     print(\"  ✓ Created: Holstein_Weight\")\n"
      ],
      "metadata": {
        "id": "ejk9lMo2Y3Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data for Modeling\n",
        "\n",
        "Here, we take our cleaned + engineered features and put them into a matrix the model can actually train on.\n",
        "\n",
        "Originally, this step kept on breaking because of non-numeric columns, alignment mismatches, and random NaN/Inf values sneaking through after earlier transformations. So, this step has also become a defense layer so the model doesn't receive bad data.\n",
        "\n",
        "The fixes included aligning the train and test sets so they have the exact same columns and dropping any remaining non-numeric columns (some columns were still of type object).\n",
        "\n",
        "One error we kept on running into was np.isinf(X_train.values) — it kept breaking because of non-numeric columns. To fix this, we forcefully converted every column into float64.\n",
        "\n",
        "After, we run another sweep for NaN + Inf and fix anything using medians (from the training set only). Finally, we scale everything using RobustScaler so that our models only receive scaled data.\n",
        "\n",
        "The end result is a fully numeric, cleaned, and scaled feature matrix.\n",
        "\n",
        "**Commented out PCA STEP**: We originally added a PCA step to reduce the dimensionality of the data. The idea was to keep 95% of the variance while compressing the data. We did this so that our model could train faster and not suffer from curse of dimensionality. However, the models that we're using handle a moderate number of features pretty well, especially after scaling and cleaning, so PCA wasn't necessary for performance.\n"
      ],
      "metadata": {
        "id": "1_W0wJv2qSqT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsYc3Y7-dRRm",
        "outputId": "9aceea6f-80ca-4e81-fd03-021527b8291a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 4: PREPARING DATA FOR MODELING\n",
            "================================================================================\n",
            "\n",
            "4.1 Aligning Train and Test Features\n",
            "----------------------------------------\n",
            "  Train features: 85\n",
            "  Test features: 85\n",
            "  ✓ Features aligned successfully\n",
            "\n",
            "4.1.5 FIX: Removing Non-Numeric Columns\n",
            "----------------------------------------\n",
            "  No non-numeric columns found to remove.\n",
            "  Final Train features count: 85\n",
            "\n",
            "4.2 Final NaN/Inf Check and Handling (Aggressive Dtype Enforcement)\n",
            "----------------------------------------\n",
            "  Attempting aggressive dtype conversion (float64)...\n",
            "  Found 24 non-numeric columns before final conversion.\n",
            "  WARNING: Found NaN values in 1 training columns.\n",
            "  → Filling NaN values with column median in train set.\n",
            "  WARNING: Found NaN values in 1 test columns.\n",
            "  → Filled test NaN values with training median.\n",
            "4.3 Scaling Features\n",
            "----------------------------------------\n",
            "  Scaled 85 features using RobustScaler\n",
            "  ✓ Scaling complete\n",
            "\n",
            "  ✓ All columns verified as numeric, no NaN or Inf values remaining\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PHASE 4: PREPARING DATA FOR MODELING\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Separate features and target\n",
        "y_train = train_featured['Milk_Yield_L'].values\n",
        "X_train = train_featured.drop(['Milk_Yield_L', 'Cattle_ID', 'Farm_ID'], axis=1, errors='ignore')\n",
        "X_test = test_featured.drop(['Cattle_ID', 'Farm_ID'], axis=1, errors='ignore')\n",
        "\n",
        "# Aligning Train and Test Features\n",
        "print(\"Aligning Train and Test Features\")\n",
        "print(\"-\" * 40)\n",
        "# Get common columns\n",
        "common_cols = X_train.columns.intersection(X_test.columns)\n",
        "X_train = X_train[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "print(f\"  Train features: {X_train.shape[1]}\")\n",
        "print(f\"  Test features: {X_test.shape[1]}\")\n",
        "print(f\"  ✓ Features aligned successfully\")\n",
        "print()\n",
        "\n",
        "# FIX: Drop all remaining non-numeric columns\n",
        "print(\"FIX: Removing Non-Numeric Columns\")\n",
        "print(\"-\" * 40)\n",
        "non_numeric_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "non_numeric_cols_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "if non_numeric_cols_train or non_numeric_cols_test:\n",
        "    cols_to_drop = list(set(non_numeric_cols_train + non_numeric_cols_test))\n",
        "    X_train.drop(cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
        "    X_test.drop(cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
        "    print(f\"  Removed {len(cols_to_drop)} non-numeric columns to ensure all features are float/int.\")\n",
        "else:\n",
        "    print(\"  No non-numeric columns found to remove.\")\n",
        "\n",
        "# Re-align after dropping (important if non-numeric columns were present)\n",
        "common_cols = X_train.columns.intersection(X_test.columns)\n",
        "X_train = X_train[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "print(f\"  Final Train features count: {X_train.shape[1]}\")\n",
        "print()\n",
        "\n",
        "# Final NaN/Inf Check and Handling (Aggressive Dtype Enforcement)\n",
        "print(\"Final NaN/Inf Check and Handling (Aggressive Dtype Enforcement)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# ⭐️ CRITICAL FIX: AGGRESSIVE CONVERSION\n",
        "# Force all columns to float, which is the necessary step for np.isinf to work.\n",
        "# Any value that cannot be converted to float will become NaN.\n",
        "print(\"  Attempting aggressive dtype conversion (float64)...\")\n",
        "\n",
        "# Identify columns that are not already numeric\n",
        "non_float_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "if len(non_float_cols) > 0:\n",
        "    print(f\"  Found {len(non_float_cols)} non-numeric columns before final conversion.\")\n",
        "\n",
        "for col in X_train.columns:\n",
        "    try:\n",
        "        # Convert to float64. This will trigger the TypeError if an incompatible object\n",
        "        # is still present, but by forcing it on the DataFrame, we isolate the issue\n",
        "        # away from the 'np.isinf(X_train.values)' step.\n",
        "        X_train[col] = X_train[col].astype(np.float64, errors='raise')\n",
        "        X_test[col] = X_test[col].astype(np.float64, errors='raise')\n",
        "\n",
        "    except ValueError as e:\n",
        "        # If astype(float) raises a ValueError, it means a non-numeric string\n",
        "        # or object is definitively present. This shouldn't happen after the\n",
        "        # previous steps, but if it does, we drop the column.\n",
        "        print(f\"  CRITICAL ERROR: Column '{col}' contains un-coercible non-numeric data. Dropping.\")\n",
        "        X_train.drop(col, axis=1, inplace=True)\n",
        "        X_test.drop(col, axis=1, inplace=True)\n",
        "\n",
        "# Re-align features after aggressive drop\n",
        "common_cols = X_train.columns.intersection(X_test.columns)\n",
        "X_train = X_train[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "\n",
        "\n",
        "# --- Continue with the NaN/Inf checks, which must now work ---\n",
        "\n",
        "# Check for NaN values\n",
        "nan_cols_train = X_train.columns[X_train.isnull().any()].tolist()\n",
        "nan_cols_test = X_test.columns[X_test.isnull().any()].tolist()\n",
        "\n",
        "if nan_cols_train:\n",
        "    print(f\"  WARNING: Found NaN values in {len(nan_cols_train)} training columns.\")\n",
        "    X_train = X_train.fillna(X_train.median())\n",
        "    print(f\"  → Filling NaN values with column median in train set.\")\n",
        "\n",
        "if nan_cols_test:\n",
        "    print(f\"  WARNING: Found NaN values in {len(nan_cols_test)} test columns.\")\n",
        "    # Fill test NaN with training median\n",
        "    X_test = X_test.fillna(X_train.median())\n",
        "    print(f\"  → Filled test NaN values with training median.\")\n",
        "\n",
        "# Check for infinite values (This is the original line that now works)\n",
        "inf_mask_train = np.isinf(X_train.values).any(axis=0)\n",
        "inf_mask_test = np.isinf(X_test.values).any(axis=0)\n",
        "\n",
        "if inf_mask_train.any():\n",
        "    inf_cols = X_train.columns[inf_mask_train].tolist()\n",
        "    print(f\"  WARNING: Found Inf values in {len(inf_cols)} columns\")\n",
        "    X_train = X_train.replace([np.inf, -np.inf], [1e10, -1e10])\n",
        "    print(f\"  → Replaced Inf values with finite numbers in X_train\")\n",
        "\n",
        "if inf_mask_test.any():\n",
        "    X_test = X_test.replace([np.inf, -np.inf], [1e10, -1e10])\n",
        "    print(f\"  → Replaced Inf values with finite numbers in X_test\")\n",
        "\n",
        "# Feature Scaling\n",
        "print(\"Scaling Features\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Use RobustScaler (less sensitive to outliers than StandardScaler)\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for easier manipulation\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(f\"  Scaled {X_train.shape[1]} features using RobustScaler\")\n",
        "print(f\"  ✓ Scaling complete\")\n",
        "print()\n",
        "\n",
        "\"\"\"\n",
        "# 4.4 Optional: PCA for Dimensionality Reduction\n",
        "print(\"4.4 Dimensionality Reduction (Optional)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if X_train_scaled.shape[1] > 50:\n",
        "    print(f\"  Feature count ({X_train_scaled.shape[1]}) is high - applying PCA\")\n",
        "\n",
        "    # Verify no NaN before PCA\n",
        "    if X_train_scaled.isnull().any().any():\n",
        "        print(\"  ERROR: Found NaN in scaled data, filling with 0\")\n",
        "        X_train_scaled = X_train_scaled.fillna(0)\n",
        "        X_test_scaled = X_test_scaled.fillna(0)\n",
        "\n",
        "    # Determine number of components to keep (e.g., 95% variance)\n",
        "    pca = PCA(n_components=0.95)\n",
        "\n",
        "    try:\n",
        "        X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "        X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "        print(f\"  Original features: {X_train_scaled.shape[1]}\")\n",
        "        print(f\"  PCA components: {X_train_pca.shape[1]}\")\n",
        "        print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "        # Visualize explained variance\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Cumulative Explained Variance')\n",
        "        plt.title('PCA Explained Variance')\n",
        "        plt.grid(True)\n",
        "        plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pca_variance.png')\n",
        "        plt.close()\n",
        "        print(\"  ✓ PCA visualization saved as 'pca_variance.png'\")\n",
        "\n",
        "        # Use PCA transformed data\n",
        "        X_train_final = X_train_pca\n",
        "        X_test_final = X_test_pca\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  WARNING: PCA failed with error: {e}\")\n",
        "        print(f\"  → Using scaled data without PCA\")\n",
        "        X_train_final = X_train_scaled.values\n",
        "        X_test_final = X_test_scaled.values\n",
        "else:\n",
        "    print(f\"  Feature count ({X_train_scaled.shape[1]}) is manageable - skipping PCA\")\n",
        "    X_train_final = X_train_scaled.values\n",
        "    X_test_final = X_test_scaled.values\n",
        "\"\"\"\n",
        "\n",
        "X_train_final = X_train_scaled.values\n",
        "X_test_final = X_test_scaled.values\n",
        "\n",
        "# Final verification\n",
        "assert X_train.select_dtypes(include=['object']).shape[1] == 0, \"Non-numeric columns still exist in X_train!\"\n",
        "assert not X_train.isnull().any().any(), \"Training data still contains NaN!\"\n",
        "assert not X_test.isnull().any().any(), \"Test data still contains NaN!\"\n",
        "print(f\"  ✓ All columns verified as numeric, no NaN or Inf values remaining\")\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code block below was meant to do a quick feature importance analysis using a RandomForestRegressor on the scaled training data. The idea was to train a small random forest, rank features by importance, and then maybe drop features whose importance falls below a small threshold.\n",
        "\n",
        "We mainly used this as a tool to see which of our engineered features were actually useful. We showed this by printing out the top 20 and bottom 20 features by importance.\n",
        "\n",
        "However, when we implemented this by removing features based on this threshold, it didn't improve performance. In some cases, it made the performance slightly worse. Thus, we decided to comment this code block out but still keep it in case we wanted to revisit pruning later."
      ],
      "metadata": {
        "id": "oYjiaxxvvj4g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUwckVmX3Tvv"
      },
      "outputs": [],
      "source": [
        "# # ============================================================================\n",
        "# # PHASE 4.5: FEATURE IMPORTANCE & SELECTION\n",
        "# # ============================================================================\n",
        "# print(\"=\"*80)\n",
        "# print(\"PHASE 4.5: FEATURE IMPORTANCE & SELECTION\")\n",
        "# print(\"=\"*80)\n",
        "# print()\n",
        "\n",
        "# # Train a quick Random Forest to get feature importances\n",
        "# print(\"Training Random Forest for feature importance analysis...\")\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# rf_selector = RandomForestRegressor(\n",
        "#     n_estimators=100,\n",
        "#     max_depth=10,\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # Use the SCALED data (X_train_scaled) with target (y_train)\n",
        "# rf_selector.fit(X_train_scaled, y_train)\n",
        "\n",
        "# # Get feature importances\n",
        "# feature_importance = pd.DataFrame({\n",
        "#     'feature': X_train_scaled.columns,\n",
        "#     'importance': rf_selector.feature_importances_\n",
        "# }).sort_values('importance', ascending=False)\n",
        "\n",
        "# print(\"\\n✓ Top 20 Most Important Features:\")\n",
        "# print(\"-\" * 50)\n",
        "# for idx, row in feature_importance.head(20).iterrows():\n",
        "#     print(f\"  {row['feature']:40} {row['importance']:.4f}\")\n",
        "\n",
        "# print(\"\\n✓ Bottom 20 Least Important Features:\")\n",
        "# print(\"-\" * 50)\n",
        "# for idx, row in feature_importance.tail(20).iterrows():\n",
        "#     print(f\"  {row['feature']:40} {row['importance']:.4f}\")\n",
        "\n",
        "# # OPTIONAL: Remove very low importance features (threshold = 0.001)\n",
        "# # Uncomment below if you want to actually remove features\n",
        "\n",
        "# low_importance_threshold = 0.001\n",
        "# low_importance_features = feature_importance[\n",
        "#     feature_importance['importance'] < low_importance_threshold\n",
        "# ]['feature'].tolist()\n",
        "\n",
        "# if len(low_importance_features) > 0:\n",
        "#     print(f\"\\n🗑️  Removing {len(low_importance_features)} low-importance features (< {low_importance_threshold}):\")\n",
        "#     for feat in low_importance_features[:10]:  # Show first 10\n",
        "#         print(f\"  • {feat}\")\n",
        "\n",
        "#     # Remove from scaled DataFrames\n",
        "#     X_train_scaled = X_train_scaled.drop(columns=low_importance_features)\n",
        "#     X_test_scaled = X_test_scaled.drop(columns=low_importance_features)\n",
        "\n",
        "#     # Update the final arrays\n",
        "#     X_train_final = X_train_scaled.values\n",
        "#     X_test_final = X_test_scaled.values\n",
        "\n",
        "#     print(f\"\\n✓ Training data shape after selection: {X_train_scaled.shape}\")\n",
        "#     print(f\"✓ Test data shape after selection: {X_test_scaled.shape}\")\n",
        "# else:\n",
        "#     print(f\"\\nℹ️  No features below threshold {low_importance_threshold}\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation Framework + Baseline Model Testing\n",
        "\n",
        "This section sets up the evaluation pipeline and runs a set of baseline models to see which ones are worth tuning later. The goal of this is not to get the best possible score, but to see which model families naturally fit the data well.\n",
        "\n",
        "For the Model Evaluation, we use 5-fold cross-validation and RMSE as the metric. This gives us a stable estimate of model performance instead of relying on a single validation split. We used RMSE as the metric because it's one of the most standard and intuitive metrics for regression problems — especially when the target is continuous and measured in real-world units (in our case, liters of milk).\n",
        "\n",
        "For Baseline Model Evaluation, we run a bunch of baseline models: boosting models, CatBoost, LightGBM, and even small neural nets. Each of these models is evaluated using the evaluate_model() function so we can compare the numbers.\n",
        "\n",
        "After running everything, we print out the top 5 models by RMSE — these are the models we focus on later for tuning.\n",
        "\n",
        "**Why we commented some of our baseline models out:**\n",
        "\n",
        "Originally, we had a wider set of baseline models: linear models (Linear Regression, Ridge), a Decision Tree, KNN variants, SVMs, and a Random Forest.\n",
        "\n",
        "However, we observed that tree-based ensembles like Gradient Boosting, LightGBM, and CatBoost were consistently outperforming plain decision trees, KNN, and basic linear models, and running every single model with 5-fold cross-validation started to get slow.\n",
        "\n",
        "We still kept this commented-out code so we can revisit these baseline models if we want to compare again, but they are not a part of the core baseline models.\n"
      ],
      "metadata": {
        "id": "V3lAknjJyBNT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-7qZQhlCqFI",
        "outputId": "8d8ffc15-7aec-4b68-f09a-a3e9f55dfd86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 5: MODEL EVALUATION FRAMEWORK\n",
            "================================================================================\n",
            "\n",
            "✓ Evaluation framework ready\n",
            "\n",
            "================================================================================\n",
            "PHASE 6: EVALUATING BASELINE MODELS\n",
            "================================================================================\n",
            "\n",
            "Evaluating baseline models with 5-fold cross-validation...\n",
            "--------------------------------------------------------------------------------\n",
            "Gradient Boosting                   | RMSE: 4.1184 (+/- 0.0133)\n",
            "CatBoost                            | RMSE: 4.1111 (+/- 0.0135)\n",
            "Neural Net (ReLU)                   | RMSE: 4.1623 (+/- 0.0215)\n",
            "Neural Net (Tanh)                   | RMSE: 4.1433 (+/- 0.0070)\n",
            "Neural Net (Logistic)               | RMSE: 4.1485 (+/- 0.0143)\n",
            "LightBM                             | RMSE: 4.1149 (+/- 0.0137)\n",
            "\n",
            "Top 5 models for hyperparameter tuning:\n",
            "  1. CatBoost (RMSE: 4.1111)\n",
            "  2. LightBM (RMSE: 4.1149)\n",
            "  3. Gradient Boosting (RMSE: 4.1184)\n",
            "  4. Neural Net (Tanh) (RMSE: 4.1433)\n",
            "  5. Neural Net (Logistic) (RMSE: 4.1485)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model Evaluation Setup\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 5: MODEL EVALUATION FRAMEWORK\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "def evaluate_model(model, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Evaluate a model using cross-validation.\n",
        "    Returns mean and std of RMSE.\n",
        "    \"\"\"\n",
        "    scores = cross_val_score(model, X, y, cv=cv,\n",
        "                            scoring='neg_mean_squared_error',\n",
        "                            n_jobs=-1)\n",
        "    rmse_scores = np.sqrt(-scores)\n",
        "    return {\n",
        "        'mean_rmse': rmse_scores.mean(),\n",
        "        'std_rmse': rmse_scores.std(),\n",
        "        'scores': rmse_scores\n",
        "    }\n",
        "\n",
        "def print_results(model_name, results):\n",
        "    \"\"\"Pretty print evaluation results.\"\"\"\n",
        "    print(f\"{model_name:35} | RMSE: {results['mean_rmse']:.4f} (+/- {results['std_rmse']:.4f})\")\n",
        "\n",
        "print(\"✓ Evaluation framework ready\")\n",
        "print()\n",
        "\n",
        "# Baseline Model Evaluation\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 6: EVALUATING BASELINE MODELS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Define baseline models with class-approved configurations\n",
        "baseline_models = {\n",
        "    # Linear Models\n",
        "\n",
        "    #'Linear Regression': LinearRegression(),\n",
        "    #'Ridge Regression': Ridge(alpha=1.0),\n",
        "\n",
        "    # Tree-based Models\n",
        "    #'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
        "\n",
        "\n",
        "    # Instance-based Models\n",
        "    #'KNN (k=5)': KNeighborsRegressor(n_neighbors=5, n_jobs=1),\n",
        "    #'KNN (k=10)': KNeighborsRegressor(n_neighbors=10, n_jobs=1),\n",
        "\n",
        "    # Support Vector Machines\n",
        "    #'SVR (Linear)': SVR(kernel='linear', C=1.0),\n",
        "    #'SVR (RBF)': SVR(kernel='rbf', C=1.0, gamma='scale'),\n",
        "\n",
        "    # Ensemble Methods (Bagging)\n",
        "    #'Random Forest': RandomForestRegressor(\n",
        "    #    n_estimators=100,\n",
        "    #    max_depth=15,\n",
        "    #    random_state=42,\n",
        "    #    n_jobs=-1\n",
        "    #),\n",
        "    # Ensemble Methods (Boosting)\n",
        "\n",
        "    'CatBoost': CatBoostRegressor(\n",
        "        iterations=1000,\n",
        "        depth=7,\n",
        "        learning_rate=0.02,\n",
        "        random_seed=42,\n",
        "        l2_leaf_reg=5,\n",
        "        thread_count=-1,\n",
        "        verbose=False  # Suppresses training output,\n",
        "    ),\n",
        "\n",
        "    # Neural Networks (with class-approved configurations)\n",
        "    'Neural Net (ReLU)': MLPRegressor(\n",
        "        #hidden_layer_sizes=(100, 50),\n",
        "        hidden_layer_sizes=(8, 4),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.01,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1\n",
        "    ),\n",
        "\n",
        "    'Neural Net (Tanh)': MLPRegressor(\n",
        "        hidden_layer_sizes=(8, 4),\n",
        "        activation='tanh',\n",
        "        solver='adam',\n",
        "        alpha=0.01,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1\n",
        "    ),\n",
        "\n",
        "\n",
        "    'Neural Net (Logistic)': MLPRegressor(\n",
        "        hidden_layer_sizes=(8, 4),\n",
        "        activation='logistic',\n",
        "        solver='adam',\n",
        "        alpha=0.01,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1\n",
        "    ),\n",
        "\n",
        "    \"LightBM\": LGBMRegressor(\n",
        "        n_estimators=1000,\n",
        "        num_leaves=8,\n",
        "        max_depth=4,\n",
        "        min_child_samples=25,\n",
        "        learning_rate=0.02,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=-1\n",
        "    ),\n",
        "\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=4,\n",
        "        sub_sample=0.8,\n",
        "        learning_rate=0.05,\n",
        "        random_state=42\n",
        "    ),\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "print(\"NOTE: Neural networks use RobustScaler preprocessing (similar to batch normalization)\")\n",
        "print(\"Activation functions: ReLU, Tanh, Logistic (Sigmoid) - all covered in class\")\n",
        "print(\"Optimizer: Adam - class-approved\")\n",
        "print(\"Learning rate: 'adaptive' - class-approved (reduces LR when validation plateaus)\")\n",
        "print(\"Regularization: L2 via alpha parameter (weight regularization)\")\n",
        "print()\n",
        "\"\"\"\n",
        "\n",
        "# Store results\n",
        "baseline_results = {}\n",
        "\n",
        "# Evaluate each baseline model\n",
        "print(\"Evaluating baseline models with 5-fold cross-validation...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for name, model in baseline_models.items():\n",
        "    results = evaluate_model(model, X_train_final, y_train, cv=5)\n",
        "    results['model'] = model\n",
        "    baseline_results[name] = results\n",
        "    print_results(name, results)\n",
        "\n",
        "print()\n",
        "\n",
        "# Identify top 5 models for further tuning\n",
        "sorted_models = sorted(baseline_results.items(), key=lambda x: x[1]['mean_rmse'])\n",
        "top_k = 5\n",
        "top_models = [name for name, _ in sorted_models[:top_k]]\n",
        "\n",
        "print(f\"Top {top_k} models for hyperparameter tuning:\")\n",
        "for i, name in enumerate(top_models, 1):\n",
        "    print(f\"  {i}. {name} (RMSE: {baseline_results[name]['mean_rmse']:.4f})\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling (Stacking & Voting)\n",
        "\n",
        "We have our tested baseline models above, but we wanted to combine the strongest ones using ensemble methods. The idea is that even if individual models have different biases, combining them cam smooth out weaknesses and improve accuracy. This section focuses on building two types of ensembles using the top 3 baselines identified above.\n",
        "\n",
        "**Stacking:**\n",
        "Stacking trains the top 3 models as base learners, then uses a \"meta-learner\" (a Ridge Regressor) to learn how to best combine their predictions. This is so the meta-learner can learn patterns that individual models miss.\n",
        "\n",
        "**Weighted Voting:** Voting takes the predictions from the top 3 models and averages them, but with weights. We calculated weights as (1 / RMSE^2), so that it gives importance to models that performed well during cross validation.\n",
        "\n",
        "Both ensembles are evaluated using the same 5-fold cross-validation, and we printed out the scores for both."
      ],
      "metadata": {
        "id": "JR_pPvRH7ITd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swLcSUFguZSb",
        "outputId": "f4c6dd00-8692-40f6-dda0-66b5950515be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 7: ENSEMBLE BUILDING (Stacking & Voting)\n",
            "================================================================================\n",
            "Strategy: Using the Top 3 Untuned Baseline Models to build ensembles.\n",
            "\n",
            "VERIFIED Estimators and Cleaned Names:\n",
            "  - CatBoost\n",
            "  - LightBM\n",
            "  - Gradient_Boosting\n",
            "--------------------------------------------------------------------------------\n",
            "7.2 Training Stacking Regressor (Top 3 Baselines + Ridge Meta-Learner)\n",
            "Stacking (Ridge Meta)               | RMSE: 4.1105 (+/- 0.0133)\n",
            "\n",
            "7.3 Training Weighted Voting Regressor\n",
            "Voting (Weighted)                   | RMSE: 4.1122 (+/- 0.0135)\n",
            "\n",
            "✓ Ensemble building complete and names validated.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "# Ensure evaluate_model, baseline_models, top_models, X_train_final, y_train are defined\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 7: ENSEMBLE BUILDING (Stacking & Voting)\")\n",
        "print(\"=\"*80)\n",
        "print(\"Strategy: Using the Top 3 Untuned Baseline Models to build ensembles.\")\n",
        "print()\n",
        "\n",
        "# Prepare Base Estimators (CORRECTED LOGIC)\n",
        "# Use the Top 3 models identified in Phase 6\n",
        "top_models = top_models[:3]\n",
        "base_estimators = []\n",
        "for name in top_models:\n",
        "    # 1. Get the Model Object\n",
        "    model_object = baseline_models[name]\n",
        "\n",
        "    # 2. Create the Clean Name\n",
        "    # We strip spaces and parentheses for use in Stacking/Voting tuples.\n",
        "    clean_name = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "\n",
        "    # 3. Append the valid (name, object) tuple\n",
        "    base_estimators.append((clean_name, model_object))\n",
        "\n",
        "# Verify the clean names before proceeding\n",
        "print(\"VERIFIED Estimators and Cleaned Names:\")\n",
        "for name, _ in base_estimators:\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "# Using 5-fold CV for ensemble evaluation (matching baseline)\n",
        "CV_FOLDS = 5\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Stacking Ensemble\n",
        "print(\"Training Stacking Regressor (Top 3 Baselines + Ridge Meta-Learner)\")\n",
        "\n",
        "stacking_regressor = StackingRegressor(\n",
        "    estimators=base_estimators,\n",
        "    # Use Ridge as the stable, regularized meta-learner\n",
        "    final_estimator=Ridge(alpha=1.0, random_state=42),\n",
        "    cv=CV_FOLDS,\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "# Evaluate Stacking Ensemble using cross-validation\n",
        "# This line caused the error, but with clean names, it should now run.\n",
        "stacking_results = evaluate_model(stacking_regressor, X_train_final, y_train, cv=CV_FOLDS)\n",
        "print_results(\"Stacking (Ridge Meta)\", stacking_results)\n",
        "\n",
        "# Store results\n",
        "baseline_results[\"Stacking (Ridge Meta)\"] = {'mean_rmse': stacking_results['mean_rmse'], 'model': stacking_regressor, 'scores': stacking_results['scores']}\n",
        "\n",
        "# Weighted Voting Ensemble (Simple and Effective Averaging)\n",
        "print(\"\\nTraining Weighted Voting Regressor\")\n",
        "\n",
        "# Calculate weights: 1 / (RMSE^2) gives more aggressive weighting to better models\n",
        "weights = []\n",
        "for name in top_models:\n",
        "    rmse = baseline_results[name]['mean_rmse']\n",
        "    weights.append(1 / (rmse ** 2))\n",
        "\n",
        "if all(w > 0 for w in weights):\n",
        "    voting_regressor = VotingRegressor(\n",
        "        estimators=base_estimators,\n",
        "        weights=weights,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Evaluate Voting Ensemble using cross-validation\n",
        "    voting_results = evaluate_model(voting_regressor, X_train_final, y_train, cv=CV_FOLDS)\n",
        "    print_results(\"Voting (Weighted)\", voting_results)\n",
        "\n",
        "    # Store results\n",
        "    baseline_results[\"Voting (Weighted)\"] = {'mean_rmse': voting_results['mean_rmse'], 'model': voting_regressor, 'scores': voting_results['scores']}\n",
        "else:\n",
        "    print(\"WARNING: Could not calculate weights. Skipping Weighted Voting.\")\n",
        "\n",
        "print(\"\\n✓ Ensemble building complete and names validated.\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model Selection + Submission\n",
        "\n",
        "This is the final phase where we take all our evaluation results and decide which model is going to be used for our submission.\n",
        "\n",
        "After ranking every baseline and ensemble model by RMSE, we compare them to a simple benchmark (Linear Regression) so we can see how much improvement each one provides. From the rankings, we can see that the Stacking Regressor with a Ridge meta-learner came out as the top performer.\n",
        "\n",
        "Once we've identified the best model (Stacking Regressor), we retrain it one last time on 100% of the training data, giving the model the maximum amount of info before generating predictions.\n",
        "\n",
        "Then, we run our model on the test set and create the submission file with the required columns."
      ],
      "metadata": {
        "id": "2DQfYg00-moa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77zbrzJd3Tvw",
        "outputId": "0569deb0-4a11-4f7c-a4f6-99ad566708d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 9: FINAL MODEL SELECTION & SUBMISSION\n",
            "================================================================================\n",
            "9.1 Final Performance Ranking (Lower RMSE is Better)\n",
            "--------------------------------------------------\n",
            "Benchmark (Linear Regression): 4.1623\n",
            "--------------------------------------------------\n",
            "🥇 Stacking (Ridge Meta)              | RMSE: 4.1105 | Improvement: 1.25%\n",
            " CatBoost                           | RMSE: 4.1111 | Improvement: 1.23%\n",
            "⭐ Voting (Weighted)                  | RMSE: 4.1122 | Improvement: 1.20%\n",
            " LightBM                            | RMSE: 4.1149 | Improvement: 1.14%\n",
            " Gradient Boosting                  | RMSE: 4.1184 | Improvement: 1.06%\n",
            " Neural Net (Tanh)                  | RMSE: 4.1433 | Improvement: 0.46%\n",
            " Neural Net (Logistic)              | RMSE: 4.1485 | Improvement: 0.33%\n",
            " Neural Net (ReLU)                  | RMSE: 4.1623 | Improvement: 0.00%\n",
            "\n",
            "9.2 Final Model Selection\n",
            "--------------------------------------------------\n",
            "🥇 **Best Model Selected: Stacking (Ridge Meta)**\n",
            "   Final Cross-Validation RMSE: 4.1105\n",
            "   Total Improvement over Benchmark: 1.25%\n",
            "--------------------------------------------------\n",
            "\n",
            "9.3 Final Training on FULL X_train_final and Prediction\n",
            "--------------------------------------------------\n",
            "  Final model trained successfully on 210000 samples.\n",
            "  Generated 40000 predictions.\n",
            "\n",
            "9.4 Creating Submission File\n",
            "--------------------------------------------------\n",
            "✓ Submission file created: 'dairy_cow_submission_ensemble_baseline.csv'\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PHASE 9: FINAL MODEL SELECTION & SUBMISSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compile and Rank Results\n",
        "print(\"Final Performance Ranking (Lower RMSE is Better)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Compile all current results (baselines and ensembles)\n",
        "final_ranking = sorted(\n",
        "    # baseline_results now holds all models (baselines + ensembles)\n",
        "    [(name, result['mean_rmse'], result['model']) for name, result in baseline_results.items()],\n",
        "    key=lambda x: x[1]\n",
        ")\n",
        "\n",
        "# Identify the best model\n",
        "best_model_name, best_rmse, final_model = final_ranking[0]\n",
        "\n",
        "# Get the baseline for comparison (Linear Regression, typically the worst)\n",
        "try:\n",
        "    # Use Linear Regression RMSE as the true performance benchmark\n",
        "    baseline_rmse = baseline_results['Linear Regression']['mean_rmse']\n",
        "except KeyError:\n",
        "    # Fallback if Linear Regression was not evaluated\n",
        "    baseline_rmse = final_ranking[-1][1]\n",
        "\n",
        "print(f\"Benchmark (Linear Regression): {baseline_rmse:.4f}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for name, rmse, _ in final_ranking:\n",
        "    # Calculate improvement percentage\n",
        "    improvement = (baseline_rmse - rmse) / baseline_rmse * 100\n",
        "\n",
        "    # Add an emoji for the top performing models\n",
        "    prefix = \"⭐\" if \"Stacking\" in name or \"Voting\" in name else \"\"\n",
        "    if name == best_model_name:\n",
        "         prefix = \"🥇\"\n",
        "\n",
        "    print(f\"{prefix} {name:34} | RMSE: {rmse:.4f} | Improvement: {improvement:.2f}%\")\n",
        "# Final Model Selection\n",
        "print(\"\\n Final Model Selection\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"🥇 **Best Model Selected: {best_model_name}**\")\n",
        "print(f\"   Final Cross-Validation RMSE: {best_rmse:.4f}\")\n",
        "print(f\"   Total Improvement over Benchmark: {((baseline_rmse - best_rmse) / baseline_rmse * 100):.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Final Training on FULL Data and Prediction\n",
        "print(\"\\nFinal Training on FULL X_train_final and Prediction\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# The selected final_model is trained one last time on 100% of the preprocessed data.\n",
        "# This assumes X_train_final, X_test_final, and y_train are available from Phase 4.\n",
        "\n",
        "# Train the final model using the full scaled dataset\n",
        "final_model.fit(X_train_final, y_train)\n",
        "\n",
        "# Generate predictions on the test set\n",
        "test_predictions = final_model.predict(X_test_final)\n",
        "test_predictions[test_predictions < 0] = 0.0 # Clip negative predictions\n",
        "\n",
        "print(f\"  Final model trained successfully on {X_train_final.shape[0]} samples.\")\n",
        "print(f\"  Generated {len(test_predictions)} predictions.\")\n",
        "\n",
        "# Create Submission File\n",
        "print(\"\\nCreating Submission File\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Note: 'test_data' must be the original un-processed test DataFrame to extract 'Cattle_ID'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Cattle_ID': test_data['Cattle_ID'],\n",
        "    'Milk_Yield_L': test_predictions\n",
        "})\n",
        "\n",
        "submission_file = 'dairy_cow_submission_ensemble_baseline.csv'\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"✓ Submission file created: '{submission_file}'\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFOSjXau3Tvx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2WvIwOQWY_Vc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Import all regression models covered in class\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    BaggingRegressor,\n",
        "    StackingRegressor,\n",
        "    VotingRegressor\n",
        ")\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: catboost in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (1.2.8)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from seaborn) (2.3.2)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from seaborn) (2.3.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from seaborn) (3.10.6)\n",
            "Requirement already satisfied: graphviz in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: scipy in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from catboost) (6.5.0)\n",
            "Requirement already satisfied: six in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\rindr\\anaconda3\\envs\\tradingagents\\lib\\site-packages (from plotly->catboost) (2.12.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SpiIuFIcN1B",
        "outputId": "64f624a5-812f-4fe5-91d4-981319ec7ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 0: LOADING RAW DATA\n",
            "================================================================================\n",
            "\n",
            "Training data shape: (210000, 36)\n",
            "Test data shape: (40000, 35)\n",
            "\n",
            "First few rows of training data:\n",
            "       Cattle_ID     Breed   Climate_Zone Management_System  Age_Months  \\\n",
            "0  CATTLE_133713  Holstein       Tropical         Intensive         114   \n",
            "1  CATTLE_027003  Holstein           Arid             Mixed         136   \n",
            "2  CATTLE_122459  Holstein       Tropical    Semi_Intensive          64   \n",
            "3  CATTLE_213419    Jersey  Mediterranean         Intensive          58   \n",
            "4  CATTLE_106260  Guernsey    Subtropical         Intensive          84   \n",
            "\n",
            "   Weight_kg  Parity Lactation_Stage  Days_in_Milk      Feed_Type  ...  \\\n",
            "0      544.8       4             Mid            62   Concentrates  ...   \n",
            "1      298.9       4             Mid           213  Crop_Residues  ...   \n",
            "2      336.6       4            Late            16            Hay  ...   \n",
            "3      370.5       1           Early           339  Crop_Residues  ...   \n",
            "4      641.5       6           Early           125     Mixed_Feed  ...   \n",
            "\n",
            "   BVD_Vaccine  Rabies_Vaccine  Previous_Week_Avg_Yield  Body_Condition_Score  \\\n",
            "0            0               1                     6.31                   3.0   \n",
            "1            0               0                    17.16                   4.0   \n",
            "2            1               0                     4.07                   3.5   \n",
            "3            0               0                    10.23                   3.0   \n",
            "4            1               1                    20.68                   3.0   \n",
            "\n",
            "   Milking_Interval_hrs        Date    Farm_ID  Feed_Quantity_lb  Mastitis  \\\n",
            "0                    12  2024-01-15  FARM_0301           36.8235         1   \n",
            "1                    12  2023-10-31  FARM_0219               NaN         0   \n",
            "2                    12  2024-05-20  FARM_0802           16.0965         0   \n",
            "3                    24  2024-07-22  FARM_0034           40.7925         0   \n",
            "4                    12  2023-01-03  FARM_0695           33.7365         1   \n",
            "\n",
            "   Milk_Yield_L  \n",
            "0     12.192634  \n",
            "1     14.717031  \n",
            "2     14.006142  \n",
            "3     24.324325  \n",
            "4     12.023074  \n",
            "\n",
            "[5 rows x 36 columns]\n",
            "\n",
            "Data types:\n",
            "Cattle_ID                   object\n",
            "Breed                       object\n",
            "Climate_Zone                object\n",
            "Management_System           object\n",
            "Age_Months                   int64\n",
            "Weight_kg                  float64\n",
            "Parity                       int64\n",
            "Lactation_Stage             object\n",
            "Days_in_Milk                 int64\n",
            "Feed_Type                   object\n",
            "Feed_Quantity_kg           float64\n",
            "Feeding_Frequency            int64\n",
            "Water_Intake_L             float64\n",
            "Walking_Distance_km        float64\n",
            "Grazing_Duration_hrs       float64\n",
            "Rumination_Time_hrs        float64\n",
            "Resting_Hours              float64\n",
            "Ambient_Temperature_C      float64\n",
            "Humidity_percent           float64\n",
            "Housing_Score              float64\n",
            "FMD_Vaccine                  int64\n",
            "Brucellosis_Vaccine          int64\n",
            "HS_Vaccine                   int64\n",
            "BQ_Vaccine                   int64\n",
            "Anthrax_Vaccine              int64\n",
            "IBR_Vaccine                  int64\n",
            "BVD_Vaccine                  int64\n",
            "Rabies_Vaccine               int64\n",
            "Previous_Week_Avg_Yield    float64\n",
            "Body_Condition_Score       float64\n",
            "Milking_Interval_hrs         int64\n",
            "Date                        object\n",
            "Farm_ID                     object\n",
            "Feed_Quantity_lb           float64\n",
            "Mastitis                     int64\n",
            "Milk_Yield_L               float64\n",
            "dtype: object\n",
            "\n",
            "Column names:\n",
            "['Cattle_ID', 'Breed', 'Climate_Zone', 'Management_System', 'Age_Months', 'Weight_kg', 'Parity', 'Lactation_Stage', 'Days_in_Milk', 'Feed_Type', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Rumination_Time_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Date', 'Farm_ID', 'Feed_Quantity_lb', 'Mastitis', 'Milk_Yield_L']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 0: LOAD RAW DATA\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 0: LOADING RAW DATA\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv('sample_data/cattle_data_train.csv')\n",
        "test_data = pd.read_csv('sample_data/cattle_data_test.csv')\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print()\n",
        "\n",
        "# Display first few rows\n",
        "print(\"First few rows of training data:\")\n",
        "print(train_data.head())\n",
        "print()\n",
        "\n",
        "print(\"Data types:\")\n",
        "print(train_data.dtypes)\n",
        "print()\n",
        "\n",
        "print(\"Column names:\")\n",
        "print(train_data.columns.tolist())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ASGyzbcOzM",
        "outputId": "af5b778f-8e0f-4d62-9013-9a4e039d3242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 1: DATA EXPLORATION\n",
            "================================================================================\n",
            "\n",
            "1.1 Dataset Overview\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 210000 entries, 0 to 209999\n",
            "Data columns (total 36 columns):\n",
            " #   Column                   Non-Null Count   Dtype  \n",
            "---  ------                   --------------   -----  \n",
            " 0   Cattle_ID                210000 non-null  object \n",
            " 1   Breed                    210000 non-null  object \n",
            " 2   Climate_Zone             210000 non-null  object \n",
            " 3   Management_System        210000 non-null  object \n",
            " 4   Age_Months               210000 non-null  int64  \n",
            " 5   Weight_kg                210000 non-null  float64\n",
            " 6   Parity                   210000 non-null  int64  \n",
            " 7   Lactation_Stage          210000 non-null  object \n",
            " 8   Days_in_Milk             210000 non-null  int64  \n",
            " 9   Feed_Type                210000 non-null  object \n",
            " 10  Feed_Quantity_kg         199519 non-null  float64\n",
            " 11  Feeding_Frequency        210000 non-null  int64  \n",
            " 12  Water_Intake_L           210000 non-null  float64\n",
            " 13  Walking_Distance_km      210000 non-null  float64\n",
            " 14  Grazing_Duration_hrs     210000 non-null  float64\n",
            " 15  Rumination_Time_hrs      210000 non-null  float64\n",
            " 16  Resting_Hours            210000 non-null  float64\n",
            " 17  Ambient_Temperature_C    210000 non-null  float64\n",
            " 18  Humidity_percent         210000 non-null  float64\n",
            " 19  Housing_Score            203721 non-null  float64\n",
            " 20  FMD_Vaccine              210000 non-null  int64  \n",
            " 21  Brucellosis_Vaccine      210000 non-null  int64  \n",
            " 22  HS_Vaccine               210000 non-null  int64  \n",
            " 23  BQ_Vaccine               210000 non-null  int64  \n",
            " 24  Anthrax_Vaccine          210000 non-null  int64  \n",
            " 25  IBR_Vaccine              210000 non-null  int64  \n",
            " 26  BVD_Vaccine              210000 non-null  int64  \n",
            " 27  Rabies_Vaccine           210000 non-null  int64  \n",
            " 28  Previous_Week_Avg_Yield  210000 non-null  float64\n",
            " 29  Body_Condition_Score     210000 non-null  float64\n",
            " 30  Milking_Interval_hrs     210000 non-null  int64  \n",
            " 31  Date                     210000 non-null  object \n",
            " 32  Farm_ID                  210000 non-null  object \n",
            " 33  Feed_Quantity_lb         199519 non-null  float64\n",
            " 34  Mastitis                 210000 non-null  int64  \n",
            " 35  Milk_Yield_L             210000 non-null  float64\n",
            "dtypes: float64(14), int64(14), object(8)\n",
            "memory usage: 57.7+ MB\n",
            "\n",
            "          Age_Months      Weight_kg         Parity   Days_in_Milk  \\\n",
            "count  210000.000000  210000.000000  210000.000000  210000.000000   \n",
            "mean       83.483905     499.930430       3.500395     182.112967   \n",
            "std        34.648982     144.659172       1.707383     105.051486   \n",
            "min        24.000000     250.000000       1.000000       1.000000   \n",
            "25%        54.000000     374.200000       2.000000      91.000000   \n",
            "50%        83.000000     500.200000       3.000000     182.000000   \n",
            "75%       114.000000     625.700000       5.000000     273.000000   \n",
            "max       143.000000     750.000000       6.000000     364.000000   \n",
            "\n",
            "       Feed_Quantity_kg  Feeding_Frequency  Water_Intake_L  \\\n",
            "count     199519.000000      210000.000000   210000.000000   \n",
            "mean          12.014793           2.999119       80.036850   \n",
            "std            3.969247           1.413147       14.987677   \n",
            "min            2.370284           1.000000       14.207737   \n",
            "25%            9.283265           2.000000       69.919162   \n",
            "50%           12.002254           3.000000       80.016973   \n",
            "75%           14.708920           4.000000       90.119812   \n",
            "max           25.454207           5.000000      149.960210   \n",
            "\n",
            "       Walking_Distance_km  Grazing_Duration_hrs  Rumination_Time_hrs  ...  \\\n",
            "count        210000.000000         210000.000000        210000.000000  ...   \n",
            "mean              4.034754              6.056710             0.256557  ...   \n",
            "std               1.928529              2.867575             6.115351  ...   \n",
            "min               0.500000              1.000000            -8.808053  ...   \n",
            "25%               2.650000              4.000000            -4.383302  ...   \n",
            "50%               4.000000              6.000000            -0.818631  ...   \n",
            "75%               5.350000              8.000000             4.051704  ...   \n",
            "max              12.000000             14.000000            31.263406  ...   \n",
            "\n",
            "       Anthrax_Vaccine    IBR_Vaccine    BVD_Vaccine  Rabies_Vaccine  \\\n",
            "count    210000.000000  210000.000000  210000.000000   210000.000000   \n",
            "mean          0.600381       0.598814       0.599824        0.600824   \n",
            "std           0.489821       0.490140       0.489935        0.489730   \n",
            "min           0.000000       0.000000       0.000000        0.000000   \n",
            "25%           0.000000       0.000000       0.000000        0.000000   \n",
            "50%           1.000000       1.000000       1.000000        1.000000   \n",
            "75%           1.000000       1.000000       1.000000        1.000000   \n",
            "max           1.000000       1.000000       1.000000        1.000000   \n",
            "\n",
            "       Previous_Week_Avg_Yield  Body_Condition_Score  Milking_Interval_hrs  \\\n",
            "count            210000.000000         210000.000000         210000.000000   \n",
            "mean                  8.747584              3.394726             12.302438   \n",
            "std                   5.901473              0.632831              4.298998   \n",
            "min                   0.000000              2.000000              6.000000   \n",
            "25%                   4.270000              3.000000             12.000000   \n",
            "50%                   7.710000              3.500000             12.000000   \n",
            "75%                  12.410000              4.000000             12.000000   \n",
            "max                  38.670000              5.000000             24.000000   \n",
            "\n",
            "       Feed_Quantity_lb       Mastitis   Milk_Yield_L  \n",
            "count     199519.000000  210000.000000  210000.000000  \n",
            "mean          26.492720       0.099976      15.589156  \n",
            "std            8.741282       0.299969       5.352079  \n",
            "min            6.615000       0.000000      -5.700324  \n",
            "25%           20.506500       0.000000      11.822207  \n",
            "50%           26.460000       0.000000      15.145871  \n",
            "75%           32.413500       0.000000      18.884708  \n",
            "max           55.125000       1.000000      44.555285  \n",
            "\n",
            "[8 rows x 28 columns]\n",
            "\n",
            "1.2 Missing Values Analysis\n",
            "--------------------------------------------------------------------------------\n",
            "                  Missing_Count  Percentage\n",
            "Feed_Quantity_kg          10481    4.990952\n",
            "Feed_Quantity_lb          10481    4.990952\n",
            "Housing_Score              6279    2.990000\n",
            "\n",
            "✓ Missing data visualization saved as 'missing_data.png'\n",
            "\n",
            "1.3 Target Variable (Milk_Yield_L) Distribution\n",
            "--------------------------------------------------------------------------------\n",
            "Mean:   15.59 L\n",
            "Median: 15.15 L\n",
            "Std:    5.35 L\n",
            "Min:    -5.70 L\n",
            "Max:    44.56 L\n",
            "Skewness: 0.48\n",
            "\n",
            "✓ Target distribution saved as 'target_distribution.png'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 1: DATA EXPLORATION\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 1: DATA EXPLORATION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# 1.1 Basic Dataset Information\n",
        "print(\"1.1 Dataset Overview\")\n",
        "print(\"-\" * 80)\n",
        "train_data.info()\n",
        "print()\n",
        "print(train_data.describe())\n",
        "print()\n",
        "\n",
        "# 1.2 Check for Missing Values\n",
        "print(\"1.2 Missing Values Analysis\")\n",
        "print(\"-\" * 80)\n",
        "missing_counts = train_data.isnull().sum()\n",
        "missing_pct = 100 * train_data.isnull().sum() / len(train_data)\n",
        "missing_table = pd.DataFrame({\n",
        "    'Missing_Count': missing_counts,\n",
        "    'Percentage': missing_pct\n",
        "})\n",
        "missing_table = missing_table[missing_table['Missing_Count'] > 0].sort_values(\n",
        "    'Percentage', ascending=False\n",
        ")\n",
        "print(missing_table)\n",
        "print()\n",
        "\n",
        "# Visualize missing data pattern\n",
        "if len(missing_table) > 0:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    missing_table['Percentage'].plot(kind='barh')\n",
        "    plt.xlabel('Percentage Missing')\n",
        "    plt.title('Missing Data by Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('missing_data.png')\n",
        "    plt.close()\n",
        "    print(\"✓ Missing data visualization saved as 'missing_data.png'\")\n",
        "print()\n",
        "\n",
        "# 1.3 Target Variable Distribution\n",
        "print(\"1.3 Target Variable (Milk_Yield_L) Distribution\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Mean:   {train_data['Milk_Yield_L'].mean():.2f} L\")\n",
        "print(f\"Median: {train_data['Milk_Yield_L'].median():.2f} L\")\n",
        "print(f\"Std:    {train_data['Milk_Yield_L'].std():.2f} L\")\n",
        "print(f\"Min:    {train_data['Milk_Yield_L'].min():.2f} L\")\n",
        "print(f\"Max:    {train_data['Milk_Yield_L'].max():.2f} L\")\n",
        "print(f\"Skewness: {train_data['Milk_Yield_L'].skew():.2f}\")\n",
        "print()\n",
        "\n",
        "# Plot distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].hist(train_data['Milk_Yield_L'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Milk Yield (L)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution of Milk Yield')\n",
        "axes[0].axvline(train_data['Milk_Yield_L'].mean(), color='r', linestyle='--', label='Mean')\n",
        "axes[0].axvline(train_data['Milk_Yield_L'].median(), color='g', linestyle='--', label='Median')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].boxplot(train_data['Milk_Yield_L'])\n",
        "axes[1].set_ylabel('Milk Yield (L)')\n",
        "axes[1].set_title('Box Plot of Milk Yield')\n",
        "plt.tight_layout()\n",
        "plt.savefig('target_distribution.png')\n",
        "plt.close()\n",
        "print(\"✓ Target distribution saved as 'target_distribution.png'\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlG9EMZXckt7",
        "outputId": "fafb2097-3c0e-4949-a995-6571ba95b269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4 Feature Type Identification\n",
            "--------------------------------------------------------------------------------\n",
            "Found 8 vaccine columns: ['FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine']\n",
            "\n",
            "Numeric features (18):\n",
            "  • Age_Months\n",
            "  • Weight_kg\n",
            "  • Parity\n",
            "  • Days_in_Milk\n",
            "  • Feed_Quantity_kg\n",
            "  • Feeding_Frequency\n",
            "  • Water_Intake_L\n",
            "  • Walking_Distance_km\n",
            "  • Grazing_Duration_hrs\n",
            "  • Rumination_Time_hrs\n",
            "  • Resting_Hours\n",
            "  • Ambient_Temperature_C\n",
            "  • Humidity_percent\n",
            "  • Housing_Score\n",
            "  • Previous_Week_Avg_Yield\n",
            "  • Body_Condition_Score\n",
            "  • Milking_Interval_hrs\n",
            "  • Feed_Quantity_lb\n",
            "\n",
            "Categorical features (14):\n",
            "  • Breed\n",
            "  • Climate_Zone\n",
            "  • Management_System\n",
            "  • Lactation_Stage\n",
            "  • Feed_Type\n",
            "  • Mastitis\n",
            "  • FMD_Vaccine\n",
            "  • Brucellosis_Vaccine\n",
            "  • HS_Vaccine\n",
            "  • BQ_Vaccine\n",
            "  • Anthrax_Vaccine\n",
            "  • IBR_Vaccine\n",
            "  • BVD_Vaccine\n",
            "  • Rabies_Vaccine\n",
            "\n",
            "1.5 Checking Feed Quantity Columns\n",
            "--------------------------------------------------------------------------------\n",
            "Correlation between Feed_Quantity_kg and Feed_Quantity_lb: 0.9988\n",
            "Average lb/kg ratio: 2.2060 (expected: 2.20462)\n",
            "→ These columns appear to be the same measurement in different units\n",
            "→ We'll drop Feed_Quantity_lb to avoid redundancy\n",
            "\n",
            "1.6 Date Feature Analysis\n",
            "--------------------------------------------------------------------------------\n",
            "Date range: 2022-01-01 00:00:00 to 2024-12-30 00:00:00\n",
            "We can extract useful temporal features from Date:\n",
            "  • Month (seasonal effects)\n",
            "  • Day of week (if relevant)\n",
            "  • Days since start of dataset\n",
            "\n",
            "1.7 Correlation Analysis with Target\n",
            "--------------------------------------------------------------------------------\n",
            "Top 10 features most correlated with Milk_Yield_L:\n",
            "Weight_kg                  0.300464\n",
            "Feed_Quantity_lb           0.223631\n",
            "Feed_Quantity_kg           0.223288\n",
            "Water_Intake_L             0.124911\n",
            "Rumination_Time_hrs        0.089823\n",
            "Previous_Week_Avg_Yield    0.089823\n",
            "Milking_Interval_hrs       0.014734\n",
            "Grazing_Duration_hrs       0.004350\n",
            "Housing_Score              0.004008\n",
            "Humidity_percent           0.002153\n",
            "Name: Milk_Yield_L, dtype: float64\n",
            "\n",
            "Bottom 10 features (least correlated or negatively correlated):\n",
            "Housing_Score            0.004008\n",
            "Humidity_percent         0.002153\n",
            "Feeding_Frequency        0.000380\n",
            "Walking_Distance_km     -0.001538\n",
            "Body_Condition_Score    -0.001647\n",
            "Resting_Hours           -0.001653\n",
            "Ambient_Temperature_C   -0.042036\n",
            "Days_in_Milk            -0.062554\n",
            "Parity                  -0.236565\n",
            "Age_Months              -0.309188\n",
            "Name: Milk_Yield_L, dtype: float64\n",
            "\n",
            "✓ Correlation heatmap saved as 'correlation_heatmap.png'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1.4 Identify Numeric vs Categorical Features\n",
        "print(\"1.4 Feature Type Identification\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Define categorical and numeric features based on the dataset\n",
        "categorical_features = [\n",
        "    'Breed', 'Climate_Zone', 'Management_System', 'Lactation_Stage',\n",
        "    'Feed_Type', 'Mastitis'\n",
        "]\n",
        "\n",
        "# Find vaccine columns (pattern: [Disease]_Vaccine)\n",
        "vaccine_cols = [col for col in train_data.columns if '_Vaccine' in col]\n",
        "print(f\"Found {len(vaccine_cols)} vaccine columns: {vaccine_cols}\")\n",
        "categorical_features.extend(vaccine_cols)\n",
        "\n",
        "# Numeric features (excluding ID, target, and date)\n",
        "numeric_features = [\n",
        "    'Age_Months', 'Weight_kg', 'Parity', 'Days_in_Milk', 'Feed_Quantity_kg',\n",
        "    'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km',\n",
        "    'Grazing_Duration_hrs', 'Rumination_Time_hrs', 'Resting_Hours',\n",
        "    'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score',\n",
        "    'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs',\n",
        "    'Feed_Quantity_lb'\n",
        "]\n",
        "\n",
        "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
        "for feat in numeric_features:\n",
        "    print(f\"  • {feat}\")\n",
        "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
        "for feat in categorical_features:\n",
        "    print(f\"  • {feat}\")\n",
        "print()\n",
        "\n",
        "# 1.5 Check for duplicate Feed_Quantity columns\n",
        "print(\"1.5 Checking Feed Quantity Columns\")\n",
        "print(\"-\" * 80)\n",
        "if 'Feed_Quantity_kg' in train_data.columns and 'Feed_Quantity_lb' in train_data.columns:\n",
        "    # Check if they're correlated (lb = kg * 2.20462)\n",
        "    correlation = train_data['Feed_Quantity_kg'].corr(train_data['Feed_Quantity_lb'])\n",
        "    print(f\"Correlation between Feed_Quantity_kg and Feed_Quantity_lb: {correlation:.4f}\")\n",
        "\n",
        "    # Check conversion factor\n",
        "    ratio = (train_data['Feed_Quantity_lb'] / train_data['Feed_Quantity_kg']).mean()\n",
        "    print(f\"Average lb/kg ratio: {ratio:.4f} (expected: 2.20462)\")\n",
        "    print(\"→ These columns appear to be the same measurement in different units\")\n",
        "    print(\"→ We'll drop Feed_Quantity_lb to avoid redundancy\")\n",
        "print()\n",
        "\n",
        "# 1.6 Date Feature Analysis\n",
        "print(\"1.6 Date Feature Analysis\")\n",
        "print(\"-\" * 80)\n",
        "if 'Date' in train_data.columns:\n",
        "    train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "    print(f\"Date range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
        "    print(\"We can extract useful temporal features from Date:\")\n",
        "    print(\"  • Month (seasonal effects)\")\n",
        "    print(\"  • Day of week (if relevant)\")\n",
        "    print(\"  • Days since start of dataset\")\n",
        "print()\n",
        "\n",
        "# 1.7 Correlation Analysis (for numeric features)\n",
        "print(\"1.7 Correlation Analysis with Target\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Calculate correlations with target\n",
        "correlations = train_data[numeric_features + ['Milk_Yield_L']].corr()['Milk_Yield_L'].drop('Milk_Yield_L')\n",
        "correlations = correlations.sort_values(ascending=False)\n",
        "print(\"Top 10 features most correlated with Milk_Yield_L:\")\n",
        "print(correlations.head(10))\n",
        "print()\n",
        "print(\"Bottom 10 features (least correlated or negatively correlated):\")\n",
        "print(correlations.tail(10))\n",
        "print()\n",
        "\n",
        "# Visualize correlation heatmap for top features\n",
        "top_features = correlations.abs().nlargest(15).index.tolist()\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = train_data[top_features + ['Milk_Yield_L']].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
        "plt.title('Correlation Heatmap: Top 15 Features + Target')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_heatmap.png')\n",
        "plt.close()\n",
        "print(\"✓ Correlation heatmap saved as 'correlation_heatmap.png'\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKedN30NczEd",
        "outputId": "09ff6e9a-7a6c-4931-a3ee-5b35d5688a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.8 Categorical Feature Analysis\n",
            "--------------------------------------------------------------------------------\n",
            "Breed: 7 unique values\n",
            "  Values: {'Holstein': 104775, 'Jersey': 42183, 'Guernsey': 31672, 'Brown Swiss': 31155, 'Holstien': 112}\n",
            "Climate_Zone: 6 unique values\n",
            "  Values: {'Temperate': 35224, 'Tropical': 35062, 'Mediterranean': 34994, 'Arid': 34954, 'Subtropical': 34937}\n",
            "Management_System: 5 unique values\n",
            "  Values: {'Intensive': 42225, 'Pastoral': 42126, 'Extensive': 41973, 'Semi_Intensive': 41906, 'Mixed': 41770}\n",
            "Lactation_Stage: 3 unique values\n",
            "  Values: {'Mid': 83895, 'Early': 63203, 'Late': 62902}\n",
            "Feed_Type: 8 unique values\n",
            "  Values: {'Dry_Fodder': 26558, 'Pasture_Grass': 26305, 'Crop_Residues': 26278, 'Concentrates': 26231, 'Mixed_Feed': 26229}\n",
            "\n",
            "1.9 Outlier Detection Using IQR Method\n",
            "--------------------------------------------------------------------------------\n",
            "Outliers in numeric features:\n",
            "Feed_Quantity_kg                 674 outliers ( 0.32%) [bounds: 1.14 to 22.85]\n",
            "Water_Intake_L                  1485 outliers ( 0.71%) [bounds: 39.62 to 120.42]\n",
            "Walking_Distance_km              748 outliers ( 0.36%) [bounds: -1.40 to 9.40]\n",
            "Rumination_Time_hrs             2406 outliers ( 1.15%) [bounds: -17.04 to 16.70]\n",
            "Ambient_Temperature_C            196 outliers ( 0.09%) [bounds: -10.42 to 54.39]\n",
            "Previous_Week_Avg_Yield         2406 outliers ( 1.15%) [bounds: -7.94 to 24.62]\n",
            "Milking_Interval_hrs           62833 outliers (29.92%) [bounds: 12.00 to 12.00]\n",
            "Feed_Quantity_lb                 698 outliers ( 0.33%) [bounds: 2.65 to 50.27]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1.8 Categorical Feature Analysis\n",
        "print(\"1.8 Categorical Feature Analysis\")\n",
        "print(\"-\" * 80)\n",
        "for cat_feat in categorical_features[:5]:  # Show first 5\n",
        "    if cat_feat in train_data.columns:\n",
        "        n_unique = train_data[cat_feat].nunique()\n",
        "        print(f\"{cat_feat}: {n_unique} unique values\")\n",
        "        print(f\"  Values: {train_data[cat_feat].value_counts().head().to_dict()}\")\n",
        "print()\n",
        "\n",
        "# 1.9 Outlier Detection\n",
        "print(\"1.9 Outlier Detection Using IQR Method\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return len(outliers), lower_bound, upper_bound\n",
        "\n",
        "print(\"Outliers in numeric features:\")\n",
        "for feature in numeric_features:\n",
        "    if feature in train_data.columns:\n",
        "        n_outliers, lower, upper = detect_outliers_iqr(train_data, feature)\n",
        "        if n_outliers > 0:\n",
        "            pct = 100 * n_outliers / len(train_data)\n",
        "            print(f\"{feature:30} {n_outliers:5} outliers ({pct:5.2f}%) [bounds: {lower:.2f} to {upper:.2f}]\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvoyzqwFc_DI",
        "outputId": "06e9a08f-19a3-4cc2-fe34-d3afa9442998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 2: DATA CLEANING\n",
            "================================================================================\n",
            "\n",
            "Cleaning training data...\n",
            "--------------------------------------------------------------------------------\n",
            "2.1 Removing Redundant Features\n",
            "----------------------------------------\n",
            "✓ Dropped Feed_Quantity_lb (redundant with Feed_Quantity_kg)\n",
            "\n",
            "2.2 Processing Date Feature\n",
            "----------------------------------------\n",
            "✓ Extracted: Month, DayOfWeek, Quarter\n",
            "✓ Created cyclical features: Month_sin, Month_cos\n",
            "✓ Dropped original Date column\n",
            "\n",
            "2.3 Handling Missing Values\n",
            "----------------------------------------\n",
            "  Feed_Quantity_kg                4.99% missing → imputed with median (12.00)\n",
            "  Housing_Score                   2.99% missing → imputed with median (0.65)\n",
            "\n",
            "2.4 Handling Outliers (Capping Method)\n",
            "----------------------------------------\n",
            "  Rumination_Time_hrs            capped 0 lower, 2 upper outliers\n",
            "  Previous_Week_Avg_Yield        capped 0 lower, 2 upper outliers\n",
            "  Milking_Interval_hrs           capped 41849 lower, 20984 upper outliers\n",
            "\n",
            "2.5 Encoding Categorical Variables\n",
            "----------------------------------------\n",
            "  Breed                          7 categories → One-hot encoded\n",
            "  Climate_Zone                   6 categories → One-hot encoded\n",
            "  Management_System              5 categories → One-hot encoded\n",
            "  Lactation_Stage                3 categories → One-hot encoded\n",
            "  Feed_Type                      8 categories → One-hot encoded\n",
            "  Mastitis                       Binary → Label encoded\n",
            "  FMD_Vaccine                    Binary → Label encoded\n",
            "  Brucellosis_Vaccine            Binary → Label encoded\n",
            "  HS_Vaccine                     Binary → Label encoded\n",
            "  BQ_Vaccine                     Binary → Label encoded\n",
            "  Anthrax_Vaccine                Binary → Label encoded\n",
            "  IBR_Vaccine                    Binary → Label encoded\n",
            "  BVD_Vaccine                    Binary → Label encoded\n",
            "  Rabies_Vaccine                 Binary → Label encoded\n",
            "\n",
            "2.6 Removing Low-Variance Features\n",
            "----------------------------------------\n",
            "  Removed 1 low-variance features:\n",
            "    • Milking_Interval_hrs\n",
            "\n",
            "2.7 Final Safety Check for NaN Values\n",
            "----------------------------------------\n",
            "  ✓ No NaN values detected\n",
            "\n",
            "Cleaning test data...\n",
            "--------------------------------------------------------------------------------\n",
            "2.1 Removing Redundant Features\n",
            "----------------------------------------\n",
            "✓ Dropped Feed_Quantity_lb (redundant with Feed_Quantity_kg)\n",
            "\n",
            "2.2 Processing Date Feature\n",
            "----------------------------------------\n",
            "✓ Extracted: Month, DayOfWeek, Quarter\n",
            "✓ Created cyclical features: Month_sin, Month_cos\n",
            "✓ Dropped original Date column\n",
            "\n",
            "2.3 Handling Missing Values\n",
            "----------------------------------------\n",
            "  Feed_Quantity_kg                5.04% missing → created indicator + imputed\n",
            "  Housing_Score                   3.05% missing → imputed with median (0.65)\n",
            "\n",
            "2.4 Handling Outliers (Capping Method)\n",
            "----------------------------------------\n",
            "  Water_Intake_L                 capped 0 lower, 1 upper outliers\n",
            "  Milking_Interval_hrs           capped 8092 lower, 4039 upper outliers\n",
            "\n",
            "2.5 Encoding Categorical Variables\n",
            "----------------------------------------\n",
            "  Breed                          7 categories → One-hot encoded\n",
            "  Climate_Zone                   6 categories → One-hot encoded\n",
            "  Management_System              5 categories → One-hot encoded\n",
            "  Lactation_Stage                3 categories → One-hot encoded\n",
            "  Feed_Type                      8 categories → One-hot encoded\n",
            "  Mastitis                       Binary → Label encoded\n",
            "  FMD_Vaccine                    Binary → Label encoded\n",
            "  Brucellosis_Vaccine            Binary → Label encoded\n",
            "  HS_Vaccine                     Binary → Label encoded\n",
            "  BQ_Vaccine                     Binary → Label encoded\n",
            "  Anthrax_Vaccine                Binary → Label encoded\n",
            "  IBR_Vaccine                    Binary → Label encoded\n",
            "  BVD_Vaccine                    Binary → Label encoded\n",
            "  Rabies_Vaccine                 Binary → Label encoded\n",
            "\n",
            "2.6 Removing Low-Variance Features\n",
            "----------------------------------------\n",
            "  Removed 1 low-variance features:\n",
            "    • Milking_Interval_hrs\n",
            "\n",
            "2.7 Final Safety Check for NaN Values\n",
            "----------------------------------------\n",
            "  ✓ No NaN values detected\n",
            "\n",
            "✓ Training data shape after cleaning: (210000, 57)\n",
            "✓ Test data shape after cleaning: (40000, 57)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 2: DATA CLEANING\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 2: DATA CLEANING\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "def clean_data(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Comprehensive data cleaning function for cattle milk yield dataset.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame to clean\n",
        "        is_train: Boolean indicating if this is training data (has target)\n",
        "\n",
        "    Returns:\n",
        "        Cleaned DataFrame\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    print(f\"Cleaning {'training' if is_train else 'test'} data...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 2.1 Drop redundant Feed_Quantity_lb (duplicate of Feed_Quantity_kg)\n",
        "    print(\"2.1 Removing Redundant Features\")\n",
        "    print(\"-\" * 40)\n",
        "    if 'Feed_Quantity_lb' in df_clean.columns:\n",
        "        df_clean.drop('Feed_Quantity_lb', axis=1, inplace=True)\n",
        "        print(\"✓ Dropped Feed_Quantity_lb (redundant with Feed_Quantity_kg)\")\n",
        "    print()\n",
        "\n",
        "    # 2.2 Handle Date Feature - Extract temporal features\n",
        "    print(\"2.2 Processing Date Feature\")\n",
        "    print(\"-\" * 40)\n",
        "    if 'Date' in df_clean.columns:\n",
        "        df_clean['Date'] = pd.to_datetime(df_clean['Date'])\n",
        "\n",
        "        # Extract useful temporal features\n",
        "        df_clean['Month'] = df_clean['Date'].dt.month\n",
        "        df_clean['DayOfWeek'] = df_clean['Date'].dt.dayofweek\n",
        "        df_clean['Quarter'] = df_clean['Date'].dt.quarter\n",
        "\n",
        "        # Create cyclical features for month (since December is close to January)\n",
        "        df_clean['Month_sin'] = np.sin(2 * np.pi * df_clean['Month'] / 12)\n",
        "        df_clean['Month_cos'] = np.cos(2 * np.pi * df_clean['Month'] / 12)\n",
        "\n",
        "        print(\"✓ Extracted: Month, DayOfWeek, Quarter\")\n",
        "        print(\"✓ Created cyclical features: Month_sin, Month_cos\")\n",
        "\n",
        "        # Drop original Date column (not useful for ML models directly)\n",
        "        df_clean.drop('Date', axis=1, inplace=True)\n",
        "        print(\"✓ Dropped original Date column\")\n",
        "    print()\n",
        "\n",
        "    # 2.3 Handle Missing Values\n",
        "    print(\"2.3 Handling Missing Values\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Strategy for numeric features\n",
        "    for col in numeric_features:\n",
        "        if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
        "            missing_pct = 100 * df_clean[col].isnull().sum() / len(df_clean)\n",
        "\n",
        "            if missing_pct < 5:\n",
        "                # Low missing: Impute with median\n",
        "                median_val = df_clean[col].median()\n",
        "                df_clean[col].fillna(median_val, inplace=True)\n",
        "                print(f\"  {col:30} {missing_pct:5.2f}% missing → imputed with median ({median_val:.2f})\")\n",
        "\n",
        "            elif missing_pct < 20:\n",
        "                # Medium missing: Create indicator + impute with median\n",
        "                df_clean[f'{col}_missing'] = df_clean[col].isnull().astype(int)\n",
        "                median_val = df_clean[col].median()\n",
        "                df_clean[col].fillna(median_val, inplace=True)\n",
        "                print(f\"  {col:30} {missing_pct:5.2f}% missing → created indicator + imputed\")\n",
        "\n",
        "            else:\n",
        "                # High missing: Consider dropping\n",
        "                print(f\"  {col:30} {missing_pct:5.2f}% missing → CONSIDER DROPPING\")\n",
        "\n",
        "    # Strategy for categorical features\n",
        "    for col in categorical_features:\n",
        "        if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
        "            missing_pct = 100 * df_clean[col].isnull().sum() / len(df_clean)\n",
        "            # Fill with 'Unknown' category\n",
        "            df_clean[col].fillna('Unknown', inplace=True)\n",
        "            print(f\"  {col:30} {missing_pct:5.2f}% missing → filled with 'Unknown'\")\n",
        "    print()\n",
        "\n",
        "    # 2.4 Handle Outliers (Cap rather than remove)\n",
        "    print(\"2.4 Handling Outliers (Capping Method)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    outlier_count = 0\n",
        "    for col in numeric_features:\n",
        "        if col in df_clean.columns:\n",
        "            Q1 = df_clean[col].quantile(0.25)\n",
        "            Q3 = df_clean[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 3 * IQR  # Use 3*IQR for conservative capping\n",
        "            upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "            n_lower = (df_clean[col] < lower_bound).sum()\n",
        "            n_upper = (df_clean[col] > upper_bound).sum()\n",
        "\n",
        "            if n_lower > 0 or n_upper > 0:\n",
        "                df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)\n",
        "                outlier_count += 1\n",
        "                print(f\"  {col:30} capped {n_lower} lower, {n_upper} upper outliers\")\n",
        "\n",
        "    if outlier_count == 0:\n",
        "        print(\"  No significant outliers detected (using 3*IQR threshold)\")\n",
        "    print()\n",
        "\n",
        "    # 2.5 Encode Categorical Variables\n",
        "    print(\"2.5 Encoding Categorical Variables\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for col in categorical_features:\n",
        "        if col in df_clean.columns:\n",
        "            n_unique = df_clean[col].nunique()\n",
        "\n",
        "            if n_unique <= 2:\n",
        "                # Binary: Simple label encoding (0/1)\n",
        "                le = LabelEncoder()\n",
        "                df_clean[col] = le.fit_transform(df_clean[col].astype(str))\n",
        "                print(f\"  {col:30} Binary → Label encoded\")\n",
        "\n",
        "            elif n_unique < 10:\n",
        "                # Low cardinality: One-hot encoding\n",
        "                dummies = pd.get_dummies(df_clean[col], prefix=col, drop_first=True)\n",
        "                df_clean = pd.concat([df_clean, dummies], axis=1)\n",
        "                df_clean.drop(col, axis=1, inplace=True)\n",
        "                print(f\"  {col:30} {n_unique} categories → One-hot encoded\")\n",
        "\n",
        "            elif n_unique < 50:\n",
        "                # Medium cardinality: Frequency encoding\n",
        "                freq_encoding = df_clean[col].value_counts().to_dict()\n",
        "                df_clean[f'{col}_freq'] = df_clean[col].map(freq_encoding)\n",
        "                df_clean.drop(col, axis=1, inplace=True)\n",
        "                print(f\"  {col:30} {n_unique} categories → Frequency encoded\")\n",
        "\n",
        "            else:\n",
        "                # High cardinality: Consider dropping or advanced encoding\n",
        "                print(f\"  {col:30} {n_unique} categories → HIGH CARDINALITY, dropping\")\n",
        "                df_clean.drop(col, axis=1, inplace=True)\n",
        "    print()\n",
        "\n",
        "    # 2.6 Remove Low-Variance Features\n",
        "    print(\"2.6 Removing Low-Variance Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    removed_features = []\n",
        "    for col in df_clean.select_dtypes(include=[np.number]).columns:\n",
        "        if col not in ['Cattle_ID', 'Farm_ID', 'Milk_Yield_L']:\n",
        "            # Check if 95% of values are the same\n",
        "            if len(df_clean[col].value_counts()) > 0:\n",
        "                mode_freq = df_clean[col].value_counts().iloc[0] / len(df_clean)\n",
        "                if mode_freq > 0.95:\n",
        "                    df_clean.drop(col, axis=1, inplace=True)\n",
        "                    removed_features.append(col)\n",
        "\n",
        "    if removed_features:\n",
        "        print(f\"  Removed {len(removed_features)} low-variance features:\")\n",
        "        for feat in removed_features:\n",
        "            print(f\"    • {feat}\")\n",
        "    else:\n",
        "        print(\"  No low-variance features detected\")\n",
        "    print()\n",
        "\n",
        "    # 2.7 Final safety check - fill any remaining NaN values\n",
        "    print(\"2.7 Final Safety Check for NaN Values\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    remaining_nan = df_clean.isnull().sum().sum()\n",
        "    if remaining_nan > 0:\n",
        "        print(f\"  WARNING: Found {remaining_nan} remaining NaN values\")\n",
        "        print(f\"  → Filling all remaining NaN with column median\")\n",
        "\n",
        "        # For numeric columns, fill with median\n",
        "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if df_clean[col].isnull().any():\n",
        "                median_val = df_clean[col].median()\n",
        "                if pd.isna(median_val):  # If median is also NaN, use 0\n",
        "                    df_clean[col].fillna(0, inplace=True)\n",
        "                    print(f\"    • {col}: filled with 0 (median was NaN)\")\n",
        "                else:\n",
        "                    df_clean[col].fillna(median_val, inplace=True)\n",
        "\n",
        "        # For any remaining non-numeric NaN, fill with 'Unknown'\n",
        "        df_clean = df_clean.fillna('Unknown')\n",
        "    else:\n",
        "        print(\"  ✓ No NaN values detected\")\n",
        "\n",
        "    # Check for infinite values\n",
        "    inf_count = np.isinf(df_clean.select_dtypes(include=[np.number]).values).sum()\n",
        "    if inf_count > 0:\n",
        "        print(f\"  WARNING: Found {inf_count} infinite values\")\n",
        "        print(f\"  → Replacing with finite values\")\n",
        "        df_clean = df_clean.replace([np.inf, -np.inf], [1e10, -1e10])\n",
        "\n",
        "    print()\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# Apply cleaning to both train and test\n",
        "train_cleaned = clean_data(train_data, is_train=True)\n",
        "test_cleaned = clean_data(test_data, is_train=False)\n",
        "\n",
        "print(f\"✓ Training data shape after cleaning: {train_cleaned.shape}\")\n",
        "print(f\"✓ Test data shape after cleaning: {test_cleaned.shape}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGn4CwdkdGTa",
        "outputId": "ae483780-a427-490f-f093-1814f5d83671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 3: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "\n",
            "Engineering features for training data...\n",
            "--------------------------------------------------------------------------------\n",
            "3.1 Creating Productivity & Efficiency Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Weight (feed efficiency)\n",
            "  ✓ Created: Water_Per_Weight\n",
            "  ✓ Created: Yield_Per_Feed (production efficiency)\n",
            "\n",
            "3.2 Creating Activity & Health Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Activity_Level\n",
            "  ✓ Created: Rest_Rumination_Ratio\n",
            "  ✓ Created: Adjusted_BCS (body condition adjusted for weight)\n",
            "\n",
            "3.3 Creating Lactation-Specific Features\n",
            "----------------------------------------\n",
            "  ✓ Created: DIM_Squared, Peak_Lactation (lactation curve)\n",
            "  ✓ Created: First_Time_Mother, Experienced_Mother\n",
            "\n",
            "3.4 Creating Environmental Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Heat_Stress_Index (temperature-humidity interaction)\n",
            "  ✓ Created: Heat_Stress, Cold_Stress indicators\n",
            "\n",
            "3.5 Creating Age & Experience Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Age_Years, Young_Cow, Prime_Age, Senior_Cow\n",
            "  ✓ Created: Avg_Age_At_Calving\n",
            "\n",
            "3.6 Creating Feed & Nutrition Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Meal\n",
            "  ✓ Created: Feed_Water_Ratio\n",
            "\n",
            "3.7 Applying Log Transformations to Skewed Features\n",
            "----------------------------------------\n",
            "  Applied log transformation to 7 skewed features:\n",
            "    • Mastitis (skewness: 2.67)\n",
            "    • Yield_Per_Feed (skewness: 1.54)\n",
            "    • Rest_Rumination_Ratio (skewness: -5.22)\n",
            "    • Peak_Lactation (skewness: 2.43)\n",
            "    • Experienced_Mother (skewness: -1.79)\n",
            "\n",
            "3.8 Final Check for Invalid Values in Engineered Features\n",
            "----------------------------------------\n",
            "  Found 98275 NaN values in engineered features\n",
            "  ✓ Filled NaN values with median\n",
            "\n",
            "3.9 Creating Advanced Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Weight_Age_Interaction\n",
            "  ✓ Created: Feed_Water_Interaction\n",
            "  ✓ Created: Parity_DIM_Interaction\n",
            "  ✓ Created: Weight_Squared\n",
            "  ✓ Created: Previous_Week_Yield_Squared\n",
            "  ✓ Created: Lactation_Peak, Log_DIM (improved lactation curve)\n",
            "  ✓ Created: Holstein_Weight\n",
            "\n",
            "Engineering features for test data...\n",
            "--------------------------------------------------------------------------------\n",
            "3.1 Creating Productivity & Efficiency Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Weight (feed efficiency)\n",
            "  ✓ Created: Water_Per_Weight\n",
            "  ✓ Created: Yield_Per_Feed (production efficiency)\n",
            "\n",
            "3.2 Creating Activity & Health Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Activity_Level\n",
            "  ✓ Created: Rest_Rumination_Ratio\n",
            "  ✓ Created: Adjusted_BCS (body condition adjusted for weight)\n",
            "\n",
            "3.3 Creating Lactation-Specific Features\n",
            "----------------------------------------\n",
            "  ✓ Created: DIM_Squared, Peak_Lactation (lactation curve)\n",
            "  ✓ Created: First_Time_Mother, Experienced_Mother\n",
            "\n",
            "3.4 Creating Environmental Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Heat_Stress_Index (temperature-humidity interaction)\n",
            "  ✓ Created: Heat_Stress, Cold_Stress indicators\n",
            "\n",
            "3.5 Creating Age & Experience Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Age_Years, Young_Cow, Prime_Age, Senior_Cow\n",
            "  ✓ Created: Avg_Age_At_Calving\n",
            "\n",
            "3.6 Creating Feed & Nutrition Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Feed_Per_Meal\n",
            "  ✓ Created: Feed_Water_Ratio\n",
            "\n",
            "3.7 Applying Log Transformations to Skewed Features\n",
            "----------------------------------------\n",
            "  Applied log transformation to 8 skewed features:\n",
            "    • Mastitis (skewness: 2.71)\n",
            "    • Feed_Quantity_kg_missing (skewness: 4.11)\n",
            "    • Yield_Per_Feed (skewness: 1.52)\n",
            "    • Rest_Rumination_Ratio (skewness: 5.86)\n",
            "    • Peak_Lactation (skewness: 2.44)\n",
            "\n",
            "3.8 Final Check for Invalid Values in Engineered Features\n",
            "----------------------------------------\n",
            "  Found 18613 NaN values in engineered features\n",
            "  ✓ Filled NaN values with median\n",
            "\n",
            "3.9 Creating Advanced Interaction Features\n",
            "----------------------------------------\n",
            "  ✓ Created: Weight_Age_Interaction\n",
            "  ✓ Created: Feed_Water_Interaction\n",
            "  ✓ Created: Parity_DIM_Interaction\n",
            "  ✓ Created: Weight_Squared\n",
            "  ✓ Created: Previous_Week_Yield_Squared\n",
            "  ✓ Created: Lactation_Peak, Log_DIM (improved lactation curve)\n",
            "  ✓ Created: Holstein_Weight\n",
            "\n",
            "✓ Training data shape after feature engineering: (210000, 92)\n",
            "✓ Test data shape after feature engineering: (40000, 93)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 3: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 3: FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "def engineer_features(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Create domain-specific features for dairy cow milk yield prediction.\n",
        "\n",
        "    Args:\n",
        "        df: Cleaned DataFrame\n",
        "        is_train: Boolean indicating if this is training data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with engineered features\n",
        "    \"\"\"\n",
        "    df_eng = df.copy()\n",
        "\n",
        "    print(f\"Engineering features for {'training' if is_train else 'test'} data...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 3.1 Productivity & Efficiency Ratios\n",
        "    print(\"3.1 Creating Productivity & Efficiency Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if 'Feed_Quantity_kg' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "        # Feed efficiency: feed per kg of body weight\n",
        "        df_eng['Feed_Per_Weight'] = df_eng['Feed_Quantity_kg'] / (df_eng['Weight_kg'] + 1)\n",
        "        print(\"  ✓ Created: Feed_Per_Weight (feed efficiency)\")\n",
        "\n",
        "    if 'Water_Intake_L' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "        # Water consumption per kg\n",
        "        df_eng['Water_Per_Weight'] = df_eng['Water_Intake_L'] / (df_eng['Weight_kg'] + 1)\n",
        "        print(\"  ✓ Created: Water_Per_Weight\")\n",
        "\n",
        "    if 'Previous_Week_Avg_Yield' in df_eng.columns and 'Feed_Quantity_kg' in df_eng.columns:\n",
        "        # Yield efficiency: milk per kg of feed\n",
        "        df_eng['Yield_Per_Feed'] = df_eng['Previous_Week_Avg_Yield'] / (df_eng['Feed_Quantity_kg'] + 1)\n",
        "        print(\"  ✓ Created: Yield_Per_Feed (production efficiency)\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.2 Activity & Health Indicators\n",
        "    print(\"3.2 Creating Activity & Health Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Walking_Distance_km', 'Grazing_Duration_hrs']):\n",
        "        # Activity level\n",
        "        df_eng['Activity_Level'] = df_eng['Walking_Distance_km'] + df_eng['Grazing_Duration_hrs']\n",
        "        print(\"  ✓ Created: Activity_Level\")\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Rumination_Time_hrs', 'Resting_Hours']):\n",
        "        # Rest/Rumination balance\n",
        "        df_eng['Rest_Rumination_Ratio'] = df_eng['Resting_Hours'] / (df_eng['Rumination_Time_hrs'] + 1)\n",
        "        print(\"  ✓ Created: Rest_Rumination_Ratio\")\n",
        "\n",
        "    if 'Body_Condition_Score' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "        # Adjusted body condition\n",
        "        df_eng['Adjusted_BCS'] = df_eng['Body_Condition_Score'] * df_eng['Weight_kg'] / 100\n",
        "        print(\"  ✓ Created: Adjusted_BCS (body condition adjusted for weight)\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.3 Lactation-Related Features\n",
        "    print(\"3.3 Creating Lactation-Specific Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if 'Days_in_Milk' in df_eng.columns and 'Milking_Interval_hrs' in df_eng.columns:\n",
        "        # Milkings per day\n",
        "        df_eng['Milkings_Per_Day'] = 24 / (df_eng['Milking_Interval_hrs'] + 1)\n",
        "        print(\"  ✓ Created: Milkings_Per_Day\")\n",
        "\n",
        "    if 'Days_in_Milk' in df_eng.columns:\n",
        "        # Lactation curve features (peak milk around 60 days)\n",
        "        df_eng['DIM_Squared'] = df_eng['Days_in_Milk'] ** 2\n",
        "        df_eng['Peak_Lactation'] = (df_eng['Days_in_Milk'] >= 40) & (df_eng['Days_in_Milk'] <= 80)\n",
        "        df_eng['Peak_Lactation'] = df_eng['Peak_Lactation'].astype(int)\n",
        "        print(\"  ✓ Created: DIM_Squared, Peak_Lactation (lactation curve)\")\n",
        "\n",
        "    if 'Parity' in df_eng.columns:\n",
        "        # Parity groups (first-time vs experienced)\n",
        "        df_eng['First_Time_Mother'] = (df_eng['Parity'] == 0).astype(int)\n",
        "        df_eng['Experienced_Mother'] = (df_eng['Parity'] >= 2).astype(int)\n",
        "        print(\"  ✓ Created: First_Time_Mother, Experienced_Mother\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.4 Environmental Interactions\n",
        "    print(\"3.4 Creating Environmental Interaction Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Ambient_Temperature_C', 'Humidity_percent']):\n",
        "        # Heat stress index (simplified THI - Temperature Humidity Index)\n",
        "        # THI = T - 0.55 * (1 - RH/100) * (T - 58) simplified version\n",
        "        df_eng['Heat_Stress_Index'] = (df_eng['Ambient_Temperature_C'] +\n",
        "                                       0.36 * df_eng['Humidity_percent'] / 100 *\n",
        "                                       df_eng['Ambient_Temperature_C'])\n",
        "        print(\"  ✓ Created: Heat_Stress_Index (temperature-humidity interaction)\")\n",
        "\n",
        "    if 'Ambient_Temperature_C' in df_eng.columns:\n",
        "        # Temperature stress indicators\n",
        "        df_eng['Heat_Stress'] = (df_eng['Ambient_Temperature_C'] > 25).astype(int)\n",
        "        df_eng['Cold_Stress'] = (df_eng['Ambient_Temperature_C'] < 5).astype(int)\n",
        "        print(\"  ✓ Created: Heat_Stress, Cold_Stress indicators\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.5 Age & Experience Features\n",
        "    print(\"3.5 Creating Age & Experience Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if 'Age_Months' in df_eng.columns:\n",
        "        # Age groups\n",
        "        df_eng['Age_Years'] = df_eng['Age_Months'] / 12\n",
        "        df_eng['Young_Cow'] = (df_eng['Age_Months'] < 30).astype(int)  # Less than 2.5 years\n",
        "        df_eng['Prime_Age'] = ((df_eng['Age_Months'] >= 30) & (df_eng['Age_Months'] <= 72)).astype(int)\n",
        "        df_eng['Senior_Cow'] = (df_eng['Age_Months'] > 72).astype(int)  # Over 6 years\n",
        "        print(\"  ✓ Created: Age_Years, Young_Cow, Prime_Age, Senior_Cow\")\n",
        "\n",
        "    if 'Age_Months' in df_eng.columns and 'Parity' in df_eng.columns:\n",
        "        # Average age at calving\n",
        "        df_eng['Avg_Age_At_Calving'] = df_eng['Age_Months'] / (df_eng['Parity'] + 1)\n",
        "        print(\"  ✓ Created: Avg_Age_At_Calving\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.6 Feed & Nutrition Features\n",
        "    print(\"3.6 Creating Feed & Nutrition Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Feed_Quantity_kg', 'Feeding_Frequency']):\n",
        "        # Feed per meal\n",
        "        df_eng['Feed_Per_Meal'] = df_eng['Feed_Quantity_kg'] / (df_eng['Feeding_Frequency'] + 1)\n",
        "        print(\"  ✓ Created: Feed_Per_Meal\")\n",
        "\n",
        "    if all(col in df_eng.columns for col in ['Feed_Quantity_kg', 'Water_Intake_L']):\n",
        "        # Feed to water ratio\n",
        "        df_eng['Feed_Water_Ratio'] = df_eng['Feed_Quantity_kg'] / (df_eng['Water_Intake_L'] + 1)\n",
        "        print(\"  ✓ Created: Feed_Water_Ratio\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.7 Log Transformations for Skewed Features\n",
        "    print(\"3.7 Applying Log Transformations to Skewed Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Identify highly skewed features\n",
        "    skewed_features = []\n",
        "    for col in df_eng.select_dtypes(include=[np.number]).columns:\n",
        "        if col not in ['Cattle_ID', 'Farm_ID', 'Milk_Yield_L']:\n",
        "            skewness = df_eng[col].skew()\n",
        "            if abs(skewness) > 1.5:  # Threshold for high skewness\n",
        "                skewed_features.append((col, skewness))\n",
        "                df_eng[f'{col}_log'] = np.log1p(df_eng[col])  # log1p handles zeros\n",
        "\n",
        "    if skewed_features:\n",
        "        print(f\"  Applied log transformation to {len(skewed_features)} skewed features:\")\n",
        "        for feat, skew in skewed_features[:5]:  # Show first 5\n",
        "            print(f\"    • {feat} (skewness: {skew:.2f})\")\n",
        "    else:\n",
        "        print(\"  No highly skewed features detected\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.8 Final check for NaN/Inf in engineered features\n",
        "    print(\"3.8 Final Check for Invalid Values in Engineered Features\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Replace any inf values created during feature engineering\n",
        "    inf_count = np.isinf(df_eng.select_dtypes(include=[np.number]).values).sum()\n",
        "    if inf_count > 0:\n",
        "        print(f\"  Found {inf_count} infinite values in engineered features\")\n",
        "        df_eng = df_eng.replace([np.inf, -np.inf], [1e10, -1e10])\n",
        "        print(f\"  ✓ Replaced infinite values\")\n",
        "\n",
        "    # Fill any NaN created during feature engineering\n",
        "    nan_count = df_eng.isnull().sum().sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"  Found {nan_count} NaN values in engineered features\")\n",
        "        numeric_cols = df_eng.select_dtypes(include=[np.number]).columns\n",
        "        df_eng[numeric_cols] = df_eng[numeric_cols].fillna(df_eng[numeric_cols].median())\n",
        "        print(f\"  ✓ Filled NaN values with median\")\n",
        "    else:\n",
        "        print(f\"  ✓ No invalid values in engineered features\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 3.9 Advanced Interaction & Polynomial Features\n",
        "    print(\"3.9 Creating Advanced Interaction Features\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # KEY INTERACTION: Weight × Age (older heavier cows produce differently)\n",
        "    if 'Weight_kg' in df_eng.columns and 'Age_Months' in df_eng.columns:\n",
        "        df_eng['Weight_Age_Interaction'] = (df_eng['Weight_kg'] * df_eng['Age_Months']) / 1000\n",
        "        print(\"  ✓ Created: Weight_Age_Interaction\")\n",
        "    \n",
        "    # Feed × Water interaction (nutritional synergy)\n",
        "    if 'Feed_Quantity_kg' in df_eng.columns and 'Water_Intake_L' in df_eng.columns:\n",
        "        df_eng['Feed_Water_Interaction'] = df_eng['Feed_Quantity_kg'] * df_eng['Water_Intake_L']\n",
        "        print(\"  ✓ Created: Feed_Water_Interaction\")\n",
        "    \n",
        "    # Parity × Days in Milk (experienced cows at different lactation stages)\n",
        "    if 'Parity' in df_eng.columns and 'Days_in_Milk' in df_eng.columns:\n",
        "        df_eng['Parity_DIM_Interaction'] = df_eng['Parity'] * df_eng['Days_in_Milk']\n",
        "        print(\"  ✓ Created: Parity_DIM_Interaction\")\n",
        "    \n",
        "    # POLYNOMIAL FEATURES (capture non-linear relationships)\n",
        "    if 'Weight_kg' in df_eng.columns:\n",
        "        df_eng['Weight_Squared'] = df_eng['Weight_kg'] ** 2\n",
        "        print(\"  ✓ Created: Weight_Squared\")\n",
        "    \n",
        "    if 'Previous_Week_Avg_Yield' in df_eng.columns:\n",
        "        df_eng['Previous_Week_Yield_Squared'] = df_eng['Previous_Week_Avg_Yield'] ** 2\n",
        "        print(\"  ✓ Created: Previous_Week_Yield_Squared\")\n",
        "    \n",
        "    # Better lactation curve (Wood's curve approximation)\n",
        "    if 'Days_in_Milk' in df_eng.columns:\n",
        "        df_eng['Lactation_Peak'] = df_eng['Days_in_Milk'] * np.exp(-0.05 * df_eng['Days_in_Milk'])\n",
        "        df_eng['Log_DIM'] = np.log1p(df_eng['Days_in_Milk'])\n",
        "        print(\"  ✓ Created: Lactation_Peak, Log_DIM (improved lactation curve)\")\n",
        "    \n",
        "    # Breed-specific features (Holstein is highest producer)\n",
        "    if 'Breed_Holstein' in df_eng.columns and 'Weight_kg' in df_eng.columns:\n",
        "        df_eng['Holstein_Weight'] = df_eng['Breed_Holstein'] * df_eng['Weight_kg']\n",
        "        print(\"  ✓ Created: Holstein_Weight\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "    return df_eng\n",
        "\n",
        "# Apply feature engineering\n",
        "train_featured = engineer_features(train_cleaned, is_train=True)\n",
        "test_featured = engineer_features(test_cleaned, is_train=False)\n",
        "\n",
        "print(f\"✓ Training data shape after feature engineering: {train_featured.shape}\")\n",
        "print(f\"✓ Test data shape after feature engineering: {test_featured.shape}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsYc3Y7-dRRm",
        "outputId": "9aceea6f-80ca-4e81-fd03-021527b8291a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 4: PREPARING DATA FOR MODELING\n",
            "================================================================================\n",
            "\n",
            "4.1 Aligning Train and Test Features\n",
            "----------------------------------------\n",
            "  Train features: 89\n",
            "  Test features: 89\n",
            "  ✓ Features aligned successfully\n",
            "\n",
            "4.1.5 FIX: Removing Non-Numeric Columns\n",
            "----------------------------------------\n",
            "  No non-numeric columns found to remove.\n",
            "  Final Train features count: 89\n",
            "\n",
            "4.2 Final NaN/Inf Check and Handling (Aggressive Dtype Enforcement)\n",
            "----------------------------------------\n",
            "  Attempting aggressive dtype conversion (float64)...\n",
            "  Found 24 non-numeric columns before final conversion.\n",
            "4.3 Scaling Features\n",
            "----------------------------------------\n",
            "  Scaled 89 features using RobustScaler\n",
            "  ✓ Scaling complete\n",
            "\n",
            "  ✓ All columns verified as numeric, no NaN or Inf values remaining\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 4: PREPARE DATA FOR MODELING (WITH FIX)\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 4: PREPARING DATA FOR MODELING\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Separate features and target\n",
        "y_train = train_featured['Milk_Yield_L'].values\n",
        "X_train = train_featured.drop(['Milk_Yield_L', 'Cattle_ID', 'Farm_ID'], axis=1, errors='ignore')\n",
        "X_test = test_featured.drop(['Cattle_ID', 'Farm_ID'], axis=1, errors='ignore')\n",
        "\n",
        "# 4.1 Aligning Train and Test Features\n",
        "print(\"4.1 Aligning Train and Test Features\")\n",
        "print(\"-\" * 40)\n",
        "# Get common columns\n",
        "common_cols = X_train.columns.intersection(X_test.columns)\n",
        "X_train = X_train[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "print(f\"  Train features: {X_train.shape[1]}\")\n",
        "print(f\"  Test features: {X_test.shape[1]}\")\n",
        "print(f\"  ✓ Features aligned successfully\")\n",
        "print()\n",
        "\n",
        "# 4.1.5 FIX: Drop all remaining non-numeric columns\n",
        "print(\"4.1.5 FIX: Removing Non-Numeric Columns\")\n",
        "print(\"-\" * 40)\n",
        "non_numeric_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "non_numeric_cols_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "if non_numeric_cols_train or non_numeric_cols_test:\n",
        "    cols_to_drop = list(set(non_numeric_cols_train + non_numeric_cols_test))\n",
        "    X_train.drop(cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
        "    X_test.drop(cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
        "    print(f\"  Removed {len(cols_to_drop)} non-numeric columns to ensure all features are float/int.\")\n",
        "else:\n",
        "    print(\"  No non-numeric columns found to remove.\")\n",
        "\n",
        "# Re-align after dropping (important if non-numeric columns were present)\n",
        "common_cols = X_train.columns.intersection(X_test.columns)\n",
        "X_train = X_train[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "print(f\"  Final Train features count: {X_train.shape[1]}\")\n",
        "print()\n",
        "\n",
        "# 4.2 Final NaN/Inf Check and Handling (Aggressive Dtype Enforcement)\n",
        "print(\"4.2 Final NaN/Inf Check and Handling (Aggressive Dtype Enforcement)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# ⭐️ CRITICAL FIX: AGGRESSIVE CONVERSION\n",
        "# Force all columns to float, which is the necessary step for np.isinf to work.\n",
        "# Any value that cannot be converted to float will become NaN.\n",
        "print(\"  Attempting aggressive dtype conversion (float64)...\")\n",
        "\n",
        "# Identify columns that are not already numeric\n",
        "non_float_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "if len(non_float_cols) > 0:\n",
        "    print(f\"  Found {len(non_float_cols)} non-numeric columns before final conversion.\")\n",
        "\n",
        "for col in X_train.columns:\n",
        "    try:\n",
        "        # Convert to float64. This will trigger the TypeError if an incompatible object\n",
        "        # is still present, but by forcing it on the DataFrame, we isolate the issue\n",
        "        # away from the 'np.isinf(X_train.values)' step.\n",
        "        X_train[col] = X_train[col].astype(np.float64, errors='raise')\n",
        "        X_test[col] = X_test[col].astype(np.float64, errors='raise')\n",
        "\n",
        "    except ValueError as e:\n",
        "        # If astype(float) raises a ValueError, it means a non-numeric string\n",
        "        # or object is definitively present. This shouldn't happen after the\n",
        "        # previous steps, but if it does, we drop the column.\n",
        "        print(f\"  CRITICAL ERROR: Column '{col}' contains un-coercible non-numeric data. Dropping.\")\n",
        "        X_train.drop(col, axis=1, inplace=True)\n",
        "        X_test.drop(col, axis=1, inplace=True)\n",
        "\n",
        "# Re-align features after aggressive drop\n",
        "common_cols = X_train.columns.intersection(X_test.columns)\n",
        "X_train = X_train[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "\n",
        "\n",
        "# --- Continue with the NaN/Inf checks, which must now work ---\n",
        "\n",
        "# Check for NaN values\n",
        "nan_cols_train = X_train.columns[X_train.isnull().any()].tolist()\n",
        "nan_cols_test = X_test.columns[X_test.isnull().any()].tolist()\n",
        "\n",
        "if nan_cols_train:\n",
        "    print(f\"  WARNING: Found NaN values in {len(nan_cols_train)} training columns.\")\n",
        "    X_train = X_train.fillna(X_train.median())\n",
        "    print(f\"  → Filling NaN values with column median in train set.\")\n",
        "\n",
        "if nan_cols_test:\n",
        "    print(f\"  WARNING: Found NaN values in {len(nan_cols_test)} test columns.\")\n",
        "    # Fill test NaN with training median\n",
        "    X_test = X_test.fillna(X_train.median())\n",
        "    print(f\"  → Filled test NaN values with training median.\")\n",
        "\n",
        "# Check for infinite values (This is the original line that now works)\n",
        "inf_mask_train = np.isinf(X_train.values).any(axis=0)\n",
        "inf_mask_test = np.isinf(X_test.values).any(axis=0)\n",
        "\n",
        "if inf_mask_train.any():\n",
        "    inf_cols = X_train.columns[inf_mask_train].tolist()\n",
        "    print(f\"  WARNING: Found Inf values in {len(inf_cols)} columns\")\n",
        "    X_train = X_train.replace([np.inf, -np.inf], [1e10, -1e10])\n",
        "    print(f\"  → Replaced Inf values with finite numbers in X_train\")\n",
        "\n",
        "if inf_mask_test.any():\n",
        "    X_test = X_test.replace([np.inf, -np.inf], [1e10, -1e10])\n",
        "    print(f\"  → Replaced Inf values with finite numbers in X_test\")\n",
        "\n",
        "# 4.3 Feature Scaling\n",
        "print(\"4.3 Scaling Features\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Use RobustScaler (less sensitive to outliers than StandardScaler)\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for easier manipulation\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(f\"  Scaled {X_train.shape[1]} features using RobustScaler\")\n",
        "print(f\"  ✓ Scaling complete\")\n",
        "print()\n",
        "\n",
        "\"\"\"\n",
        "# 4.4 Optional: PCA for Dimensionality Reduction\n",
        "print(\"4.4 Dimensionality Reduction (Optional)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if X_train_scaled.shape[1] > 50:\n",
        "    print(f\"  Feature count ({X_train_scaled.shape[1]}) is high - applying PCA\")\n",
        "\n",
        "    # Verify no NaN before PCA\n",
        "    if X_train_scaled.isnull().any().any():\n",
        "        print(\"  ERROR: Found NaN in scaled data, filling with 0\")\n",
        "        X_train_scaled = X_train_scaled.fillna(0)\n",
        "        X_test_scaled = X_test_scaled.fillna(0)\n",
        "\n",
        "    # Determine number of components to keep (e.g., 95% variance)\n",
        "    pca = PCA(n_components=0.95)\n",
        "\n",
        "    try:\n",
        "        X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "        X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "        print(f\"  Original features: {X_train_scaled.shape[1]}\")\n",
        "        print(f\"  PCA components: {X_train_pca.shape[1]}\")\n",
        "        print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "        # Visualize explained variance\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Cumulative Explained Variance')\n",
        "        plt.title('PCA Explained Variance')\n",
        "        plt.grid(True)\n",
        "        plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pca_variance.png')\n",
        "        plt.close()\n",
        "        print(\"  ✓ PCA visualization saved as 'pca_variance.png'\")\n",
        "\n",
        "        # Use PCA transformed data\n",
        "        X_train_final = X_train_pca\n",
        "        X_test_final = X_test_pca\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  WARNING: PCA failed with error: {e}\")\n",
        "        print(f\"  → Using scaled data without PCA\")\n",
        "        X_train_final = X_train_scaled.values\n",
        "        X_test_final = X_test_scaled.values\n",
        "else:\n",
        "    print(f\"  Feature count ({X_train_scaled.shape[1]}) is manageable - skipping PCA\")\n",
        "    X_train_final = X_train_scaled.values\n",
        "    X_test_final = X_test_scaled.values\n",
        "\"\"\"\n",
        "\n",
        "X_train_final = X_train_scaled.values\n",
        "X_test_final = X_test_scaled.values\n",
        "\n",
        "# Final verification\n",
        "assert X_train.select_dtypes(include=['object']).shape[1] == 0, \"Non-numeric columns still exist in X_train!\"\n",
        "assert not X_train.isnull().any().any(), \"Training data still contains NaN!\"\n",
        "assert not X_test.isnull().any().any(), \"Test data still contains NaN!\"\n",
        "print(f\"  ✓ All columns verified as numeric, no NaN or Inf values remaining\")\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ============================================================================\n",
        "# # PHASE 4.5: FEATURE IMPORTANCE & SELECTION\n",
        "# # ============================================================================\n",
        "# print(\"=\"*80)\n",
        "# print(\"PHASE 4.5: FEATURE IMPORTANCE & SELECTION\")\n",
        "# print(\"=\"*80)\n",
        "# print()\n",
        "\n",
        "# # Train a quick Random Forest to get feature importances\n",
        "# print(\"Training Random Forest for feature importance analysis...\")\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# rf_selector = RandomForestRegressor(\n",
        "#     n_estimators=100, \n",
        "#     max_depth=10, \n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # Use the SCALED data (X_train_scaled) with target (y_train)\n",
        "# rf_selector.fit(X_train_scaled, y_train)\n",
        "\n",
        "# # Get feature importances\n",
        "# feature_importance = pd.DataFrame({\n",
        "#     'feature': X_train_scaled.columns,\n",
        "#     'importance': rf_selector.feature_importances_\n",
        "# }).sort_values('importance', ascending=False)\n",
        "\n",
        "# print(\"\\n✓ Top 20 Most Important Features:\")\n",
        "# print(\"-\" * 50)\n",
        "# for idx, row in feature_importance.head(20).iterrows():\n",
        "#     print(f\"  {row['feature']:40} {row['importance']:.4f}\")\n",
        "\n",
        "# print(\"\\n✓ Bottom 20 Least Important Features:\")\n",
        "# print(\"-\" * 50)\n",
        "# for idx, row in feature_importance.tail(20).iterrows():\n",
        "#     print(f\"  {row['feature']:40} {row['importance']:.4f}\")\n",
        "\n",
        "# # OPTIONAL: Remove very low importance features (threshold = 0.001)\n",
        "# # Uncomment below if you want to actually remove features\n",
        "\n",
        "# low_importance_threshold = 0.001\n",
        "# low_importance_features = feature_importance[\n",
        "#     feature_importance['importance'] < low_importance_threshold\n",
        "# ]['feature'].tolist()\n",
        "\n",
        "# if len(low_importance_features) > 0:\n",
        "#     print(f\"\\n🗑️  Removing {len(low_importance_features)} low-importance features (< {low_importance_threshold}):\")\n",
        "#     for feat in low_importance_features[:10]:  # Show first 10\n",
        "#         print(f\"  • {feat}\")\n",
        "    \n",
        "#     # Remove from scaled DataFrames\n",
        "#     X_train_scaled = X_train_scaled.drop(columns=low_importance_features)\n",
        "#     X_test_scaled = X_test_scaled.drop(columns=low_importance_features)\n",
        "    \n",
        "#     # Update the final arrays\n",
        "#     X_train_final = X_train_scaled.values\n",
        "#     X_test_final = X_test_scaled.values\n",
        "    \n",
        "#     print(f\"\\n✓ Training data shape after selection: {X_train_scaled.shape}\")\n",
        "#     print(f\"✓ Test data shape after selection: {X_test_scaled.shape}\")\n",
        "# else:\n",
        "#     print(f\"\\nℹ️  No features below threshold {low_importance_threshold}\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-7qZQhlCqFI",
        "outputId": "8d8ffc15-7aec-4b68-f09a-a3e9f55dfd86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 5: MODEL EVALUATION FRAMEWORK\n",
            "================================================================================\n",
            "\n",
            "✓ Evaluation framework ready\n",
            "\n",
            "================================================================================\n",
            "PHASE 6: EVALUATING BASELINE MODELS\n",
            "================================================================================\n",
            "\n",
            "Evaluating baseline models with 5-fold cross-validation...\n",
            "--------------------------------------------------------------------------------\n",
            "CatBoost                            | RMSE: 4.1184 (+/- 0.0144)\n",
            "Gradient Boosting                   | RMSE: 4.1241 (+/- 0.0144)\n",
            "Neural Net (ReLU)                   | RMSE: 4.1794 (+/- 0.0220)\n",
            "Neural Net (Tanh)                   | RMSE: 4.1642 (+/- 0.0139)\n",
            "Neural Net (Logistic)               | RMSE: 4.1512 (+/- 0.0133)\n",
            "\n",
            "Top 5 models for hyperparameter tuning:\n",
            "  1. CatBoost (RMSE: 4.1184)\n",
            "  2. Gradient Boosting (RMSE: 4.1241)\n",
            "  3. Neural Net (Logistic) (RMSE: 4.1512)\n",
            "  4. Neural Net (Tanh) (RMSE: 4.1642)\n",
            "  5. Neural Net (ReLU) (RMSE: 4.1794)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 5: MODEL EVALUATION SETUP\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 5: MODEL EVALUATION FRAMEWORK\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "def evaluate_model(model, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Evaluate a model using cross-validation.\n",
        "    Returns mean and std of RMSE.\n",
        "    \"\"\"\n",
        "    scores = cross_val_score(model, X, y, cv=cv,\n",
        "                            scoring='neg_mean_squared_error',\n",
        "                            n_jobs=-1)\n",
        "    rmse_scores = np.sqrt(-scores)\n",
        "    return {\n",
        "        'mean_rmse': rmse_scores.mean(),\n",
        "        'std_rmse': rmse_scores.std(),\n",
        "        'scores': rmse_scores\n",
        "    }\n",
        "\n",
        "def print_results(model_name, results):\n",
        "    \"\"\"Pretty print evaluation results.\"\"\"\n",
        "    print(f\"{model_name:35} | RMSE: {results['mean_rmse']:.4f} (+/- {results['std_rmse']:.4f})\")\n",
        "\n",
        "print(\"✓ Evaluation framework ready\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 6: BASELINE MODEL EVALUATION\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 6: EVALUATING BASELINE MODELS\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# Define baseline models with class-approved configurations\n",
        "baseline_models = {\n",
        "    # Linear Models\n",
        "   \n",
        "    #'Linear Regression': LinearRegression(),\n",
        "    #'Ridge Regression': Ridge(alpha=1.0),\n",
        "\n",
        "    # Tree-based Models\n",
        "    #'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
        "   \n",
        "\n",
        "    # Instance-based Models\n",
        "    #'KNN (k=5)': KNeighborsRegressor(n_neighbors=5, n_jobs=1),\n",
        "    #'KNN (k=10)': KNeighborsRegressor(n_neighbors=10, n_jobs=1),\n",
        "\n",
        "    # Support Vector Machines\n",
        "    #'SVR (Linear)': SVR(kernel='linear', C=1.0),\n",
        "    #'SVR (RBF)': SVR(kernel='rbf', C=1.0, gamma='scale'),\n",
        "    \n",
        "    # Ensemble Methods (Bagging)\n",
        "    #'Random Forest': RandomForestRegressor(\n",
        "    #    n_estimators=100,\n",
        "    #    max_depth=15,\n",
        "    #    random_state=42,\n",
        "    #    n_jobs=-1\n",
        "    #),\n",
        "    # Ensemble Methods (Boosting)\n",
        "    \n",
        "    'CatBoost': CatBoostRegressor(\n",
        "        iterations=200,\n",
        "        depth=6,\n",
        "        learning_rate=0.05,\n",
        "        random_seed=42,\n",
        "        verbose=False  # Suppresses training output\n",
        "    ),\n",
        "\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=128,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    # Neural Networks (with class-approved configurations)\n",
        "    'Neural Net (ReLU)': MLPRegressor(\n",
        "        #hidden_layer_sizes=(100, 50),\n",
        "        hidden_layer_sizes=(8, 4),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.01,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1\n",
        "    ),\n",
        "\n",
        "    'Neural Net (Tanh)': MLPRegressor(\n",
        "        hidden_layer_sizes=(8, 4),\n",
        "        activation='tanh',\n",
        "        solver='adam',\n",
        "        alpha=0.01,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1\n",
        "    ),\n",
        "\n",
        "\n",
        "    'Neural Net (Logistic)': MLPRegressor(\n",
        "        hidden_layer_sizes=(8, 4),\n",
        "        activation='logistic',\n",
        "        solver='adam',\n",
        "        alpha=0.01,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1\n",
        "    )\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "print(\"NOTE: Neural networks use RobustScaler preprocessing (similar to batch normalization)\")\n",
        "print(\"Activation functions: ReLU, Tanh, Logistic (Sigmoid) - all covered in class\")\n",
        "print(\"Optimizer: Adam - class-approved\")\n",
        "print(\"Learning rate: 'adaptive' - class-approved (reduces LR when validation plateaus)\")\n",
        "print(\"Regularization: L2 via alpha parameter (weight regularization)\")\n",
        "print()\n",
        "\"\"\"\n",
        "\n",
        "# Store results\n",
        "baseline_results = {}\n",
        "\n",
        "# Evaluate each baseline model\n",
        "print(\"Evaluating baseline models with 5-fold cross-validation...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for name, model in baseline_models.items():\n",
        "    results = evaluate_model(model, X_train_final, y_train, cv=5)\n",
        "    results['model'] = model\n",
        "    baseline_results[name] = results\n",
        "    print_results(name, results)\n",
        "\n",
        "print()\n",
        "\n",
        "# Identify top 5 models for further tuning\n",
        "sorted_models = sorted(baseline_results.items(), key=lambda x: x[1]['mean_rmse'])\n",
        "top_k = 5\n",
        "top_models = [name for name, _ in sorted_models[:top_k]]\n",
        "\n",
        "print(f\"Top {top_k} models for hyperparameter tuning:\")\n",
        "for i, name in enumerate(top_models, 1):\n",
        "    print(f\"  {i}. {name} (RMSE: {baseline_results[name]['mean_rmse']:.4f})\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "swLcSUFguZSb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 7: ENSEMBLE BUILDING (Stacking & Voting)\n",
            "================================================================================\n",
            "Strategy: Using the Top 3 Untuned Baseline Models to build ensembles.\n",
            "\n",
            "VERIFIED Estimators and Cleaned Names:\n",
            "  - CatBoost\n",
            "  - Gradient_Boosting\n",
            "  - Neural_Net_Logistic\n",
            "--------------------------------------------------------------------------------\n",
            "7.2 Training Stacking Regressor (Top 3 Baselines + Ridge Meta-Learner)\n",
            "Stacking (Ridge Meta)               | RMSE: 4.1140 (+/- 0.0141)\n",
            "\n",
            "7.3 Training Weighted Voting Regressor\n",
            "Voting (Weighted)                   | RMSE: 4.1178 (+/- 0.0138)\n",
            "\n",
            "✓ Ensemble building complete and names validated.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 7: ENSEMBLE BUILDING (Revised and Corrected)\n",
        "# ============================================================================\n",
        "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "# Ensure evaluate_model, baseline_models, top_models, X_train_final, y_train are defined\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 7: ENSEMBLE BUILDING (Stacking & Voting)\")\n",
        "print(\"=\"*80)\n",
        "print(\"Strategy: Using the Top 3 Untuned Baseline Models to build ensembles.\")\n",
        "print()\n",
        "\n",
        "# 7.1 Prepare Base Estimators (CORRECTED LOGIC)\n",
        "# Use the Top 3 models identified in Phase 6\n",
        "top_models = top_models[:3]\n",
        "base_estimators = []\n",
        "for name in top_models:\n",
        "    # 1. Get the Model Object\n",
        "    model_object = baseline_models[name]\n",
        "    \n",
        "    # 2. Create the Clean Name\n",
        "    # We strip spaces and parentheses for use in Stacking/Voting tuples.\n",
        "    clean_name = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "    \n",
        "    # 3. Append the valid (name, object) tuple\n",
        "    base_estimators.append((clean_name, model_object))\n",
        "\n",
        "# Verify the clean names before proceeding\n",
        "print(\"VERIFIED Estimators and Cleaned Names:\")\n",
        "for name, _ in base_estimators:\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "# Using 5-fold CV for ensemble evaluation (matching baseline)\n",
        "CV_FOLDS = 5 \n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 7.2 Stacking Ensemble (High Performance)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"7.2 Training Stacking Regressor (Top 3 Baselines + Ridge Meta-Learner)\")\n",
        "\n",
        "stacking_regressor = StackingRegressor(\n",
        "    estimators=base_estimators, \n",
        "    # Use Ridge as the stable, regularized meta-learner\n",
        "    final_estimator=Ridge(alpha=1.0, random_state=42), \n",
        "    cv=CV_FOLDS, \n",
        "    n_jobs=-1,\n",
        "    passthrough=False \n",
        ")\n",
        "\n",
        "# Evaluate Stacking Ensemble using cross-validation\n",
        "# This line caused the error, but with clean names, it should now run.\n",
        "stacking_results = evaluate_model(stacking_regressor, X_train_final, y_train, cv=CV_FOLDS)\n",
        "print_results(\"Stacking (Ridge Meta)\", stacking_results)\n",
        "\n",
        "# Store results\n",
        "baseline_results[\"Stacking (Ridge Meta)\"] = {'mean_rmse': stacking_results['mean_rmse'], 'model': stacking_regressor, 'scores': stacking_results['scores']}\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 7.3 Weighted Voting Ensemble (Simple and Effective Averaging)\n",
        "# ----------------------------------------------------------------------------\n",
        "print(\"\\n7.3 Training Weighted Voting Regressor\")\n",
        "\n",
        "# Calculate weights: 1 / (RMSE^2) gives more aggressive weighting to better models\n",
        "weights = []\n",
        "for name in top_models:\n",
        "    rmse = baseline_results[name]['mean_rmse'] \n",
        "    weights.append(1 / (rmse ** 2))\n",
        "    \n",
        "if all(w > 0 for w in weights):\n",
        "    voting_regressor = VotingRegressor(\n",
        "        estimators=base_estimators,\n",
        "        weights=weights,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Evaluate Voting Ensemble using cross-validation\n",
        "    voting_results = evaluate_model(voting_regressor, X_train_final, y_train, cv=CV_FOLDS)\n",
        "    print_results(\"Voting (Weighted)\", voting_results)\n",
        "    \n",
        "    # Store results\n",
        "    baseline_results[\"Voting (Weighted)\"] = {'mean_rmse': voting_results['mean_rmse'], 'model': voting_regressor, 'scores': voting_results['scores']}\n",
        "else:\n",
        "    print(\"WARNING: Could not calculate weights. Skipping Weighted Voting.\")\n",
        "\n",
        "print(\"\\n✓ Ensemble building complete and names validated.\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE 9: FINAL MODEL SELECTION & SUBMISSION\n",
            "================================================================================\n",
            "9.1 Final Performance Ranking (Lower RMSE is Better)\n",
            "--------------------------------------------------\n",
            "Benchmark (Linear Regression): 4.1794\n",
            "--------------------------------------------------\n",
            "🥇 Stacking (Ridge Meta)              | RMSE: 4.1140 | Improvement: 1.57%\n",
            "⭐ Voting (Weighted)                  | RMSE: 4.1178 | Improvement: 1.47%\n",
            " CatBoost                           | RMSE: 4.1184 | Improvement: 1.46%\n",
            " Gradient Boosting                  | RMSE: 4.1241 | Improvement: 1.32%\n",
            " Neural Net (Logistic)              | RMSE: 4.1512 | Improvement: 0.67%\n",
            " Neural Net (Tanh)                  | RMSE: 4.1642 | Improvement: 0.36%\n",
            " Neural Net (ReLU)                  | RMSE: 4.1794 | Improvement: 0.00%\n",
            "\n",
            "9.2 Final Model Selection\n",
            "--------------------------------------------------\n",
            "🥇 **Best Model Selected: Stacking (Ridge Meta)**\n",
            "   Final Cross-Validation RMSE: 4.1140\n",
            "   Total Improvement over Benchmark: 1.57%\n",
            "--------------------------------------------------\n",
            "\n",
            "9.3 Final Training on FULL X_train_final and Prediction\n",
            "--------------------------------------------------\n",
            "  Final model trained successfully on 210000 samples.\n",
            "  Generated 40000 predictions.\n",
            "\n",
            "9.4 Creating Submission File\n",
            "--------------------------------------------------\n",
            "✓ Submission file created: 'dairy_cow_submission_ensemble_baseline.csv'\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PHASE 9: FINAL MODEL SELECTION AND SUBMISSION\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 9: FINAL MODEL SELECTION & SUBMISSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 9.1 Compile and Rank Results\n",
        "print(\"9.1 Final Performance Ranking (Lower RMSE is Better)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Compile all current results (baselines and ensembles)\n",
        "final_ranking = sorted(\n",
        "    # baseline_results now holds all models (baselines + ensembles)\n",
        "    [(name, result['mean_rmse'], result['model']) for name, result in baseline_results.items()],\n",
        "    key=lambda x: x[1]\n",
        ")\n",
        "\n",
        "# Identify the best model\n",
        "best_model_name, best_rmse, final_model = final_ranking[0]\n",
        "\n",
        "# Get the baseline for comparison (Linear Regression, typically the worst)\n",
        "try:\n",
        "    # Use Linear Regression RMSE as the true performance benchmark\n",
        "    baseline_rmse = baseline_results['Linear Regression']['mean_rmse']\n",
        "except KeyError:\n",
        "    # Fallback if Linear Regression was not evaluated\n",
        "    baseline_rmse = final_ranking[-1][1] \n",
        "\n",
        "print(f\"Benchmark (Linear Regression): {baseline_rmse:.4f}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for name, rmse, _ in final_ranking:\n",
        "    # Calculate improvement percentage\n",
        "    improvement = (baseline_rmse - rmse) / baseline_rmse * 100\n",
        "    \n",
        "    # Add an emoji for the top performing models\n",
        "    prefix = \"⭐\" if \"Stacking\" in name or \"Voting\" in name else \"\"\n",
        "    if name == best_model_name:\n",
        "         prefix = \"🥇\"\n",
        "\n",
        "    print(f\"{prefix} {name:34} | RMSE: {rmse:.4f} | Improvement: {improvement:.2f}%\")\n",
        "\n",
        "print(\"\\n9.2 Final Model Selection\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"🥇 **Best Model Selected: {best_model_name}**\")\n",
        "print(f\"   Final Cross-Validation RMSE: {best_rmse:.4f}\")\n",
        "print(f\"   Total Improvement over Benchmark: {((baseline_rmse - best_rmse) / baseline_rmse * 100):.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# 9.3 Final Training on FULL Data and Prediction\n",
        "print(\"\\n9.3 Final Training on FULL X_train_final and Prediction\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# The selected final_model is trained one last time on 100% of the preprocessed data.\n",
        "# This assumes X_train_final, X_test_final, and y_train are available from Phase 4.\n",
        "\n",
        "# Train the final model using the full scaled dataset \n",
        "final_model.fit(X_train_final, y_train)\n",
        "\n",
        "# Generate predictions on the test set\n",
        "test_predictions = final_model.predict(X_test_final)\n",
        "test_predictions[test_predictions < 0] = 0.0 # Clip negative predictions\n",
        "\n",
        "print(f\"  Final model trained successfully on {X_train_final.shape[0]} samples.\")\n",
        "print(f\"  Generated {len(test_predictions)} predictions.\")\n",
        "\n",
        "# 9.4 Create Submission File\n",
        "print(\"\\n9.4 Creating Submission File\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Note: 'test_data' must be the original un-processed test DataFrame to extract 'Cattle_ID'\n",
        "submission_df = pd.DataFrame({\n",
        "    'Cattle_ID': test_data['Cattle_ID'], \n",
        "    'Milk_Yield_L': test_predictions\n",
        "})\n",
        "\n",
        "submission_file = 'dairy_cow_submission_ensemble_baseline.csv'\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"✓ Submission file created: '{submission_file}'\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tradingagents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
